# <p align="right">3. 从张量开始</p>
***

>本章包括：  
&emsp;&emsp;- 理解张量--PyTorch中的基本数据结构  
&emsp;&emsp;- 张量的索引和运算  
&emsp;&emsp;- 与NumPy多维数组的交互操作  
&emsp;&emsp;- 将计算转移到GPU上以提高速度 

&emsp;&emsp;在上一章中，我们介绍了深度学习所实现的众多应用中的一部分。它们无一例外地包括获取某种形式的数据(如图像或文本)，并产生另一种形式的数据(如标签、数字或更多图像、文本)。从这个角度来看，深度学习其实就是构建一个能够将数据从一种表示形式转化为另一种表示形式的系统。这种转换是通过从一系列展示所需映射的例子中提取共性来驱动的。例如，系统可能会注意到狗的一般形状和金毛犬的典型颜色。通过结合这两种图像属性，系统可以将具有给定形状和颜色的图像正确地映射到**金毛犬**标签上，而不是**黑色的实验室**(或就此而言，黄褐色的山猫猫)。由此产生的系统可以消耗大量的类似输入，并为这些输入产生有意义的输出。

&emsp;&emsp;该过程首先将我们的输入转换为浮点数。我们将在第4章中介绍如何将图像像素转换为数字，如图3.1中的第一步所示(以及许多其他类型的数据)。但在这之前，在本章中，我们将学习如何在 PyTorch 中使用 **tensors (张量)** 来处理所有的浮点数。

## 3.1 <span id='chap3-1'>浮点数世界</span>

由于浮点数是网络处理信息的方式，我们需要一种方法将我们想要处理的现实世界的数据编码成网络可以消化的东西，然后将输出的数据解码成我们可以理解和使用的内容。

<center>图3.1 深度神经网络学习如何将输入表示转化为输出表示。(注：神经元和输出的数量不成比例。)</center>

<div align=center>
<img src="../img/Chap3/3-1.png" />
</div>

深度神经网络通常会分阶段学习从一种形式的数据到另一种形式的数据的转换，这意味着每个阶段之间的部分转换数据可以被认为是一系列中间表示。对于图像识别来说，早期的表示可以是边缘检测或某些纹理（如毛皮）等。更深层次的表示可以捕获更复杂的结构，如耳朵、鼻子或眼睛。

一般来说，这种中间表示是浮点数的集合，它对输入进行表征，并以一种有助于描述输入如何映射到神经网络的输出的方式来捕获数据的结构。这种表征是针对当前任务的，可以从相关示例中学习的。这些浮点数的集合及其操作是现代人工智能的核心--我们将在本书中看到几个这样的示例。

请务必牢记，这些中间表示（如图3.1第二步所示）是将输入与前一层神经元的权重相结合的结果。每个中间表示对于之前的输入都是唯一的。

在开始将数据转换为浮点输入之前，我们首先必须对PyTorch如何处理和存储数据--作为输入、中间表示和输出--有一些扎实的了解。本章将专门讨论这个问题。

为此，PyTorch引入了一个基本的数据结构：**tensor (张量)**。我们在第2章对预训练网络进行推理时，已经碰到了张量。对于那些来自数学、物理学或工程学的人来说，张量这个术语与空间、参照系以及它们之间的变换的概念捆绑在一起。无论好坏，这些概念在这里均不适用。在深度学习的上下文中，**张量是指向量和矩阵到任意维数的泛化**，如图3.2所示。这一概念的另一个名称是**multidimensional array (多维数组)**。张量的维数与用于引用张量内标量值的索引数一致。

<center>图3.2 张量是在PyTorch中表示数据的基础</center>

<div align=center>
<img src="../img/Chap3/3-2.png" />
</div>


PyTorch不是唯一处理多维数组的库。NumPy是迄今为止最受欢迎的多维数组库，可以说它现在已经成为数据科学的通用语言。PyTorch的特点是与NumPy的无缝互操作性，这为它带来了与Python中其他科学库的一流集成，比如SciPy (https://www.scipy.org) 、Scikit-Learning (https://scikit-learn.org) 和Pandas (https://pandas.pydata.org) 。

与NumPy数组相比，PyTorch张量具有一些超强的功能，例如能够在图形处理单元(GPU)上执行非常快速的操作、在多个设备或机器上分布式操作，以及跟踪创建它们的计算图的能力。这些都是实现现代深度学习库的重要功能。

在本章中，我们将介绍PyTorch张量，其中涵盖了它的相关基础知识，以便为本书其余部分的工作做准备。首先，我们将学习如何使用PyTorch张量库操纵张量。这包括诸如如何将数据存储在内存中，如何在固定时间内对任意大的张量执行某些操作以及前面提到的NumPy互操作性和GPU加速。如果要使张量成为我们编程工具箱中的首选工具，了解张量的功能和API是非常重要的。在下一章中，我们将充分利用这些知识，并学习如何以能够通过神经网络进行学习的方式表示几种不同类型的数据。

## 3.2 <span id='chap3-2'>张量：多维数组</span>

我们已经了解到，张量是PyTorch中的基本数据结构。 张量是一个数组：即一种存储数字集合的数据结构，这些数字可以使用索引单独访问，并且可以使用多个索引进行索引。

### 3.2.1 从Python列表到PyTorch张量

让我们看看列表索引的实际应用，这样我们就可以将其与张量索引进行比较。在Python中取一个包含三个数字的列表(.code/p1ch3/1_tensors.ipynb)：

```python
# In[1]:
a = [1.0, 2.0, 1.0]
```

我们可以使用对应的从零开始的索引来访问列表的第一个元素：

```python
# In[2]:
a[0]
 
# Out[2]:
1.0
 
# In[3]:
a[2] = 3.0
a
 
# Out[3]:
[1.0, 2.0, 3.0]
```

对于处理数字向量的简单 Python 程序来说，使用 Python 列表来存储向量是很常见的，比如二维线的坐标。正如我们将在下一章中所看到的，使用更有效的张量数据结构，可以表示许多类型的数据--从图像到时间序列，甚至是句子。通过定义张量上的操作(我们将在本章中探讨其中一些操作)，即使是使用高级（但不是特别快速）的语言（例如Python），我们也可以同时高效地切片和操作数据。

### 3.2.2 构建我们的第一个张量

让我们构造第一个PyTorch张量，并看看它的构造。它不必是一个特别有意义的张量，只需设置为包括三个1的列即可：

```python
# In[4]:
import torch
a = torch.ones(3)
a
 
# Out[4]:
tensor([1., 1., 1.])
 
# In[5]:
a[1]
 
# Out[5]:
tensor(1.)
 
# In[6]:
float(a[1])
 
# Out[6]:
1.0
 
# In[7]:
a[2] = 2.0
a
 
# Out[7]:
tensor([1., 1., 2.])
```

```python
其中，
# In[4]:
import torch       <------导入import模块
a = torch.ones(3)  <------创建尺寸为3且填充值为1的一维张量
...
...
```

导入`torch`模块后，我们调用一个函数来创建一个大小为3的（一维）张量，填充值为1.0。我们可以使用它从零开始的索引来访问一个元素，或者为其赋一个新值。虽然从表面上看，这个例子与数字对象的列表没有太大区别，但在本质上是完全不同的。

## 3.2.3 张量的本质

Python列表或数字元组是在内存中单独分配的Python对象的集合，如图 3.3 左侧所示。另一方面，PyTorch tensors或NumPy数组从视图上看，(通常)是分布在连续内存块上，包含有*unboxed C*数字类型(译者注：即不涉及指针或堆分配的类型)，而不是Python对象。在这种情况下，如图3.3右侧所示，每个元素都是一个32位(4 字节)的浮点数。这意味着存储1,000,000个浮点数的1D(一维)张量恰好需要4,000,000个连续的字节，外加元数据(如尺寸和数值类型)的少量开销。

<center>图3.3 Python对象(装箱)数值与张量(未装箱数组)数值的对比</center>

<div align=center>
<img src="../img/Chap3/3-3.png" />
</div>

假设我们有一个要用来表示几何对象的坐标列表：可能是一个顶点位于坐标(4，1)、(5，3)和(2，1)的2D三角形。这个例子与深度学习不是特别相关，但是很容易理解。我们可以使用一维张量，将X存储在偶数索引中，将Y存储在奇数索引中，而不是像我们前面所做的那样，在Python列表中将坐标作为数字，如下所示：

```python
# In[8]:
points = torch.zeros(6)
points[0] = 4.0
points[1] = 1.0
points[2] = 5.0
points[3] = 3.0
points[4] = 2.0
points[5] = 1.0
```

```python
其中，
# In[8]:
points = torch.zeros(6)  <-----使用.zeros只是一种获得合适尺寸数组的方法。
points[0] = 4.0          <-----我们用实际使用的值重写这些0
```

我们还可以将Python列表传递给构造函数，效果相同：

```python
# In[9]:
points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])
points
 
# Out[9]:
tensor([4., 1., 5., 3., 2., 1.])
```

为了得到第一个点的坐标，我们做如下操作：

```python
# In[10]:
float(points[0]), float(points[1])
 
# Out[10]:
(4.0, 1.0)
```

这是可以的，尽管让第一个索引引用单个二维点而不是点坐标是可行的。为此，我们可以使用2D张量：

```python
# In[11]:
points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])
points
 
# Out[11]:
tensor([[4., 1.],
        [5., 3.],
        [2., 1.]])
```

在这里，我们向构造函数传递一串列表。我们可以查询张量的形状：

```python
# In[12]:
points.shape
 
# Out[12]:
torch.Size([3, 2])
```

这让我们知道了张量在每个维度上的尺寸。我们也可以使用`zeros`或`ones`来初始化张量，以元组形式提供尺寸：

```python
# In[13]:
points = torch.zeros(3, 2)
points
 
# Out[13]:
tensor([[0., 0.],
        [0., 0.],
        [0., 0.]])
```

现在我们可以使用两个索引访问张量中的单个元素：

```python
# In[14]:
points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])
points
 
# Out[14]:
tensor([[4., 1.],
        [5., 3.],
        [2., 1.]])
 
# In[15]:
points[0, 1]
 
# Out[15]:
tensor(1.)
```

这将返回我们数据集中第0个点的Y坐标。我们还可以像以前一样访问张量中的第一个元素，以获取第一个点的2D坐标：

```python
# In[16]:
points[0]
 
# Out[16]:
tensor([4., 1.])
```

输出的是另一个张量，它展示了同一基础数据的不同视图。新的张量是一个尺寸为2的1D张量，引用了`points`张量中第一行的值。这是否意味着分配了一个新的内存块，将值复制到其中，然后新的内存被包裹在一个新的张量对象中返回？不，因为这样的效率很低，尤其是当我们有数百万个点的时候。我们将在本章稍后的3.7节介绍张量的视图时，重新讨论张量的存储方式。

## 3.3 <span id='chap3-3'>索引张量</span>











## 3.4 <span id='chap3-4'>命名张量</span>
## 3.5 <span id='chap3-5'>张量元素类型</span>
## 3.6 <span id='chap3-6'>张量API</span>
## 3.7 <span id='chap3-7'>张量：存储的风景</span>
## 3.8 <span id='chap3-8'>张量元数据：尺寸、偏移和步长</span>
## 3.9 <span id='chap3-9'>移动张量至GPU</span>
## 3.10 <span id='chap3-10'>NumPy互操作性</span>
## 3.11 <span id='chap3-11'>广义张量也是张量</span>
## 3.12 <span id='chap3-12'>序列化张量</span>
## 3.13 <span id='chap3-13'>结论</span>
## 3.14 <span id='chap3-14'>练习</span>
## 3.15 <span id='chap3-15'>小结</span>

---