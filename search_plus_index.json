{"./":{"url":"./","title":"关于本项目","keywords":"","body":"Deep-Learning-with-PyTorch-Chinese 深度学习与PyTorch（中文版）-paper2Fox 特别鸣谢：本项目受ShusenTang的开源项目（基本摘要版）启发而进行，旨在完成对完整版书籍的翻译。 本项目（链接）预计将PyTorch官方书籍《Deep learning with PyTorch》翻译成中文。目前该书在PyTorch官网可以免费领取（传送门）,也可以在Manning订购正版（传送门）。 This project aims to translate the PyTorch official book Deep learning with PyTorch into Chinese. 1. 书籍简介 自 2016 年诞生以来，PyTorch 已经成为当今最火热的深度学习框架之一。最近，官方权威的 PyTorch 教程书《Deep learning with PyTorch》终于问世了，消息一出就获得巨佬 Yann LeCun 力荐，是入门PyTorch及深度学习的绝佳教材。作者：ELI STEVENS, LUCA ANTIGA, AND THOMAS VIEHMANN 目前，PyTorch官网提供的PDF已经是2020年7月正式出版的完整版，共522页，内容共包括以下三大部分： 核心PyTorch（第1-8章） 从现实世界中的图像中学习：肺癌的早期发现（第9-14章） 部署（第15章） 该书提供了详细的动手入门，介绍了如何使用流行的开源机器学习框架PyTorch构建和训练神经网络。内容涵盖： 深度学习和PyTorch库简介 预训练网络 张量 学习机制 使用神经网络拟合数据 使用卷积泛化 真实示例：建立用于癌症检测的神经网络 部署到生产 原书的GitHub链接提供了可运行代码和数据集下载，本项目不再另行搬运，请读者们移步原贴。 Manning购买下载链接: https://www.manning.com/books/deep-learning-with-pytorch Amazon购买下载链接: https://amzn.to/38Iwrff (affiliate link) 原书勘误可以在manning 查找, 或者查看 https://deep-learning-with-pytorch.github.io/dlwpt-code/errata.html 2. 项目简介 本项目计划将原书翻译成中文，目前正持续更新中。 《深度学习与PyTorch》 面向对PyTorch感兴趣，尤其是想快速入门PyTorch的童鞋。本项目并不要求你有任何深度学习或者机器学习的背景知识，你只需了解基础的数学和编程，如基础的线性代数、微分和概率，以及基础的Python编程。相比于其他的参考型教材，这本书是通过逐步深入进而掌握PyTorch的不二之选。 本仓库的master分支将提供markdown格式的中文翻译，后续会推出可供下载的版本。目前暂时仅提供Gitbook在线预览。 在线预览： https://paper2fox.github.io/Deep-Learning-with-PyTorch-Chinese/ 欢迎对本项目做出贡献或提出issue。 3. 目录 关于本项目 关于本书 关于作者 关于封面 Part 1 核心PyTorch 1. 深度学习和PyTorch库简介 1.1 深度学习革命 1.2 深度学习：PyTorch 1.3 为什么使用深度学习 1.4 概述PyTorch如何支持深度学习项目 1.5 硬件和软件要求 1.6 练习 1.7 小结 2. 预训练网络 2.1 一种可识别图像主体的预训练网络 2.2 一种从伪到真的预训练模型 2.3 一种描述场景的预训练网络 2.4 Torch Hub 2.5 结论 2.6 练习 2.7 小结 3. 从张量开始 3.1 浮点数世界 3.2 张量：多维数组 3.3 索引张量 3.4 命名张量 3.5 张量元素类型 3.6 张量API 3.7 张量：存储的风景 3.8 张量元数据：尺寸、偏移和步长 3.9 移动张量至GPU 3.10 NumPy互操作性 3.11 广义张量也是张量 3.12 序列化张量 3.13 结论 3.14 练习 3.15 小结 未完待续 4. 声明 译者纯粹出于学习目的与个人兴趣而进行翻译，不追求任何经济利益； 本项目仅限于学习研究目的的使用，译者保留对此项目的署名权，任何转载必须注明出处，但不得用于任何商业用途； 使用本项目对原著的侵权行为或者违反知识产权保护法的任何行为，与译者无关； 有能力阅读英文书籍者请阅读原版或购买完整版书籍。 LICENSE CC BY-NC（署名-非商业性使用）4.0 By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "},"about/about_this_book.html":{"url":"about/about_this_book.html","title":"关于本书","keywords":"","body":"关于本书 本书的目的是利用PyTorch提供深度学习的基础，并在实际项目中展现它们。我们致力于解释深度学习的关键概念，并演示如何通过PyTorch将它们教给实践者。在这本书中，我们尝试提供能进一步探索的直觉概念，同时，我们会有选择地深剖细节，展示这些场景背后的道理。 《深度学习与PyTorch》并不是一本参考手册，相反，它是一本概念性的读物，可以让您独立地在线探索更高级的材料。因此，我们将重点关注 PyTorch 提供的子集功能。最值得注意的是递归神经网络，但是PyTorch API的其他部分也是如此。 本书的读者是谁？ 本书面向正在或打算成为深度学习实践者并希望了解PyTorch的开发人员。我们假设我们的典型读者是计算机科学家、数据科学家，软件工程师，或者是相关课程的本科生或更高学历的学生。由于我们假定读者不具备深度学习的先验知识，因此本书前半部分的某些内容可能是有经验的从业者已经知道的重复概念。对于这类读者，我们希望本书能提供一个与您已知主题稍有不同的视角。 我们希望读者具备命令式和面向对象编程的基本知识。由于本书使用Python，您应该熟悉其语法和操作环境。了解如何在您选择的平台上安装Python包和运行脚本是先决条件。来自C++、Java、JavaScript、Ruby或其他类似语言的读者应该很容易上手，但在本书之外还需要做一些补充。同样，虽然不是硬性要求，熟悉NumPy也会很有帮助。我们还希望您能熟悉一些基本的线性代数，例如知道什么是矩阵和向量，什么是点积。 本书的组织结构：路线图 《深度学习与PyTorch》分为三个不同的部分。第1部分涵盖了基础知识，而第2部分在第1部分介绍的基本概念的基础上，加入更多高级概念，引导您完成一个端到端项目。简短的第3部分则以了解PyTorch提供的部署功能来结束本书。您可能会注意到各部分之间存在不同的语言和图片风格。尽管本书是无休止地协作规划、讨论和编辑的结果，但写作和绘制图片的工作却是由各位作者分担完成的。Luca主要负责第1部分，Eli负责第2部分2，Thomas则试图将第3部分的风格与前两部分相融合。与其追求最低限度的统一性，我们决定保留各部分特有的原貌。     以下是各部分的分解，并简要介绍了各部分。 PART 1 在第1部分中，我们将迈出使用PyTorch的第一步，培养理解开发PyTorch项目所需的基本技能，并开始构建我们自己的项目。我们将介绍PyTorch API和一些令PyTorch成为库的幕后特性，并着手训练一个初始的分类模型。在第1部分结束时，我们将为处理一个真实的项目而做好准备。 第1章介绍了PyTorch作为一个库及其在深度学习革命中的地位，并触及了PyTorch有别于其他深度学习框架的地方。 第2章通过运行预训练网络的示例来展示PyTorch的运行情况，它演示了如何在PyTorch Hub中下载和运行模型。 第3章介绍了PyTorch的基本构件--张量，展示了它的API，并在介绍了一些幕后的实现细节。 第4章演示了如何将不同类型的数据表示为张量，以及深度学习模型对张量形状的预期。 第5章介绍了通过梯度下降进行学习的机制，以及PyTorch如何使其实现自动微分功能。 第6章展示了在PyTorch中使用nn和optim模块构建和训练回归神经网络的过程。 第7章在上一章的基础上，创建一个用于图像分类的全连接模型，并扩展PyTorch API的知识。 第8章介绍了卷积神经网络，并扩展了用于构建神经网络模型及其PyTorch实现的更多高级概念。 PART 2 在第2部分中，每一章都使我们更接近自动检测肺癌的综合解决方案。我们将以这一难题为动力，展示解决癌症筛查等大规模问题所需的实际方法。这是一个专注于清洁工程、故障排除和问题解决的大型项目。 第9章描述了我们将用于肺部肿瘤分类的端到端策略，从计算机断层扫描(CT)成像开始。 第10章使用标准PyTorch API加载人体注释数据以及来自CT扫描的图像，并将相关信息转换为张量。 第11章介绍了一个使用第10章中介绍的训练数据的一级分类模型，我们对该模型进行了训练，并收集了基本的性能指标。 我们还介绍了如何使用TensorBoard来监控训练。 第12章探索并实现了标准的性能指标，并使用这些指标来判别训练模型的缺陷。然后，我们通过使用数据平衡和数据增强的改进训练集来缓解这些缺陷。 第13章描述了分割，这是一种像素到像素的模型架构，我们使用它来生成覆盖整个CT扫描图像的可能肿瘤位置的热图。 这张热图可以用于在未经人类判别的CT扫描图片上寻找肿瘤位置。 第14章实现了最终的端到端项目：使用我们的新型分割模型对癌症患者进行诊断，然后进行分类。 PART 3 第3部分是有关部署的单章。 第15章概述了如何将PyTorch模型部署到简单的Web服务里，如何将其嵌入C++程序或将其移植到手机中。 关于代码 本书中的所有代码都是为Python 3.6或更高版本编写的。该书的代码可从Manning的网站(https://www.manning.com/books/deep-learning-with-pytorch) 和GitHub(https://github.com/deep-learning-withpytorch/dlwpt-code) 下载。版本3.6.8是撰写本文时的最新版本，也是我们用来测试本书中示例的版本。 例如： $ python Python 3.6.8 (default, Jan 14 2019, 11:02:34) [GCC 8.0.1 20180414 on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 在Bash提示符下输入的命令行以$开头（例如，本示例中的$ python行）。固定宽度的内联代码看起来像self。 以>>>开头的代码块是Python交互式提示符下会话的记录。>>>字符不应视为输入；本书中不以>>>或者...开头的文本行都是输出。在某些情况下，会在>>>前插入额外的空行，以提高打印时的可读性。当您在交互提示符下实际输入文本时，不包括这些空行： >>> print(\"Hello, world!\") Hello, world! >> print(\"Until next time...\") Until next time... 我们还大量使用了Jupyter Notebook，如第1章第1.5.1节所述。 作为官方GitHub存储库的一部分，我们提供的notebook中的代码如下所示： # In[1]: print(\"Hello, world!\") # Out[1]: Hello, world! # In[2]: print(\"Until next time...\") # Out[2]: Until next time... 几乎我们所有的示例笔记本都在第一个单元格中包含以下样板（在前几章中可能会遗漏一些行），之后，我们会将其略过： # In[1]: %matplotlib inline from matplotlib import pyplot as plt import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.set_printoptions(edgeitems=2) torch.manual_seed(123) 此外，代码块是.py源文件的部分或全部。 Listing 15.1 main.py:5, def man def main(): print(\"Hello, world!\") if __name__ == '__main__': main() 本书中的许多代码样本都是以两格缩进的方式呈现的。由于印刷的局限性，代码列表每行被限制在80个字符内，这对于大量缩进的代码段来说可能不切实际。使用两个空格缩进有助于减轻原本会出现的过度换行问题。所有可供本书下载的代码(同样，在 https://www.manning.com/books/deep-learningwith-pytorch 和 https://github.com/deep-learning-with-pytorch/dlwpt-code) 都使用了一致的四个空格缩进。以_t为后缀命名的变量是存储在CPU内存中的张量，_g是存储在GPU内存中的张量，_a是NumPy数组。 硬件和软件需求 第1部分被设计为不需要任何特定的计算资源。任何最新的计算机或在线计算资源都是足够的。同样，也不需要特定的操作系统。在第2部分中，我们预计完成更高级示例的完整培训运行将需要支持一个支持CUDA的GPU。第2部分中使用的设备默认参数均假设GPU具有8 GB的RAM(我们建议使用NVIDIA GTX 1070或更高版本)，但是如果您的硬件可用的RAM较少，可以调整这些参数。第2部分的癌症检测项目所需的原始数据下载量约为60 GB，系统上总共需要200 GB(至少)可用磁盘空间用于训练模型。幸运的是，在线计算服务最近开始免费提供GPU时间。我们将在相应的章节中更详细地讨论计算需求。 您需要Python 3.6或更高版本的版本；可在Python网站(https://www.python.org/downloads) 上找到相关说明。有关 PyTorch的安装信息，请参阅 PyTorch 官方网站 (https://pytorch.org/get-started/locally) 上的入门指南。我们建议Windows用户使用Anaconda或Miniconda (https://www.anaconda.com/distribution 或 https://docs.conda.io/en/latest/miniconda.html) 进行安装。其他操作系统如Linux通常有更多的可行选项，Pip是Python最常用的包管理器。我们提供了一个requirements.txt文件，Pip可以用它来处理Python的依赖需求。由于目前的苹果笔记本电脑不包含支持CUDA的GPU，因此PyTorch的macOS预编译包只支持CPU。当然，有经验的用户可以自由地以最符合您首选开发环境的方式安装软件包。 liveBook论坛 购买《深度学习与PyTorch》包括免费访问由Manning Publications运营的私人网络论坛，您可以在该论坛上对本书发表评论，提出技术问题，并从作者和其他用户那里获得帮助。要访问该论坛，请访问 https://livebook.manning.com/#！/book/deep-learning-with-pytorch/discussion 。您可以在 https://livebook.manning.com/#！/discussion 了解更多关于Manning论坛和行为规则的信息。Manning对读者承诺提供一个场所，让个人读者之间和读者与作者之间可以进行有意义的对话。这不是对作者的任何具体参与量的承诺，他们对论坛的贡献仍然是自愿的（且无偿的）。我们建议您尝试向他们提出一些具有挑战性的问题，以免他们失去兴趣。只要该书还在印刷，就可以从出版商的网站上访问论坛并获得以前讨论的存档。 其他在线资源 虽然这本书没有假设读者需要具备深度学习的先验知识，但它没有包括深度学习的基础介绍。我们涵盖了基础知识，但我们的重点是熟练使用PyTorch库。我们鼓励感兴趣的读者在阅读本书之前、期间或之后建立起对深度学习的直观理解。为此，Grokking Deep Learning (https://www.manning.com/books/grokking-deep-learning) 是一本很好的资源，它可以帮助我们对深度神经网络的基础机制建立起强大的心理模型和直觉理解。要想获得全面的介绍和参考，我们建议您阅读Goodfellow et al.的Deep Learning (https://www.deeplearningbook.org)。当然，Manning Publications也具有大量的深度学习书目(https://www.manning.com/CATALOG#SECTION-83)，涵盖了该领域的各种主题。根据您的兴趣，它们中的许多都值得成为您阅读的下一本书。 2 Eli和Thomas的一些艺术作品出现在了其他部分；如果您发现在某个章节中，风格改变了，不要感到震惊！ By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "},"about/about_the_authors.html":{"url":"about/about_the_authors.html","title":"关于作者","keywords":"","body":"关于作者 伊莱·史蒂文斯(Eli Stevens)职业生涯的大部分时间都在硅谷的初创公司工作，职位从软件工程师(制造企业网络设备)到CTO(开发放射肿瘤学软件)不一而足。在本书出版之际，他正在研究自动驾驶汽车行业的机器学习。 卢卡·安提加(Luca Antiga)在2000年代从事生物医学工程的研究工作，并在过去十年中担任一家人工智能工程公司的联合创始人兼CTO。他曾为多个开源项目做出贡献，包括PyTorch核心。他最近在美国联合创办了一家创业公司，专注于数据定义软件的基础设施。 托马斯·维曼(Thomas Viehmann)是一名机器学习和PyTorch专业培训师与顾问，常驻德国慕尼黑，同时也是PyTorch核心开发人员。拥有数学博士学位的他并不畏惧理论，但在将理论应用于计算挑战时，他非常实际。 By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "},"about/about_the_cover_illustration.html":{"url":"about/about_the_cover_illustration.html","title":"关于封面","keywords":"","body":"关于封面 《深度学习与PyTorch》封面上的人物标题为Kardian。插图取自雅克·格拉塞特·德圣·索维(Jacques Grasset de Saint-Sauveur,1757-1810)所著的各国服饰收藏集，题为《Costumes civils actuels de tous les peuples connus》，1788年在法国出版。其中的每幅插图都是手工精细绘制和上色。Grasset de Saint-Sauveur 的收藏品种类丰富，生动地提醒我们，就在200年前，世界上的城镇和地区在文化上是多么的分离。人们彼此隔绝，说着不同的方言和语言。无论是在街上还是在乡下，只要根据他们的着装，就很容易辨别出他们住在哪里，他们的职业或生活地位是什么。 从那时起，我们的着装方式发生了变化，当年如此丰富的地区差异也逐渐消失了。现在很难区分不同大陆的居民，更不用说不同的城镇、地区或国家了。也许我们已经用文化多样性换取了更为多样化的个人生活，当然也换成了更加多样化和快节奏的科技生活。 在很难区分这本和那本书的时候，Manning用基于两个世纪前丰富多彩的地区生活的书籍封面来颂扬计算机行业的创造性和主动性,封面通过格拉塞特·德·圣·索维的画作重现生机。 By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "},"Chapter1/1.0.html":{"url":"Chapter1/1.0.html","title":"Part 1 核心PyTorch","keywords":"","body":"Part 1 核心PyTorch   欢迎阅读本书的第一部分。这是我们使用PyTorch的第一步，获得理解其剖析结构和解决PyTorch项目机制所需的基本技能。   在第1章中，我们将首次接触PyTorch，了解它是什么，它能解决什么问题，以及它与其他深度学习框架的关系。第2章将引导我们了解全貌，让我们有机会玩玩那些已经预先训练好的有趣任务的模型。第3章的内容稍微严肃一些，讲授PyTorch程序中使用的基本数据结构：张量。第4章将带我们进行另一次漫游，这一次将讲述来自不同领域的数据表示为PyTorch张量的方法。第5章揭示了程序如何从示例中学习，以及PyTorch是如何支持这一过程的。第6章介绍了什么是神经网络以及如何使用PyTorch构建神经网络。第7章使用神经网络架构解决了一个简单的图像分类问题。最后，第8章展示了如何使用卷积神经网络以更智能的方式解决同样的问题。   在第一部分结束时，我们将了解如何在第二部分中使用PyTorch解决实际问题。 By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "},"Chapter1/1.1.html":{"url":"Chapter1/1.1.html","title":"1. 深度学习和PyTorch库简介","keywords":"","body":"1. 深度学习和PyTorch库简介 本章包括：  - 深度学习如何改变我们的机器学习方法  - 理解为什么Pythorch适合深度学习  - 检查典型的深度学习项目  - 您需要遵循的硬件以及示例   人工智能这个定义不明确的术语涵盖了一系列学科，这些学科大量的研究、审查、混淆、幻想炒作和科幻恐惧症的影响。当然，现实要乐观得多。如果断言今天的机器正在学习任何人类意义上的“思考(think)”，这是不真实的。相反，我们已经发现了一类通用的算法，它们能够非常、非常有效地逼近复杂的、非线性的过程，我们可以用它们来自动执行以前仅限于人类完成的任务。   例如，在 https://inferkit.com/ 上有一种称为GPT-2的语言模型可以一次生成一个单词的连贯文本段落。当我们把这一段写进去时，它产生了以下内容： 接下来，我们要从电子邮件地址的语料库中输入一个短语列表，看看程序是否能将这些列表解析为句子。同样，这比本篇文章开头的搜索要复杂得多,但希望能帮助您理解用各种编程语言构建句子结构的基础知识。 即使在杂乱无章的背后没有一个明确的主题,对于一台机器来说，这是非常连贯的。 更令人印象深刻的是，执行这些以前只有人类才能完成的任务的能力是“通过示例”获得的，而不是由人类编写一套手工制作的规则。在某种程度上，我们逐步认识到，智能是一个我们经常与自我意识混为一谈的概念，而自我意识绝对不是成功完成这类任务所必需的。归根结底，计算机智能的问题可能根本不重要。Edsger W. Dijkstra发现，机器是否能思考的问题 \"和潜艇是否会游泳的问题本质上相关。”1 我们正在讨论的这类算法属于深度学习的人工智能(AI)子类别，它通过提供有启发性的例子来训练名为深度神经网络的数学实体。深度学习使用大量数据来近似输入和输出相距甚远的复杂函数，例如输入是一张图像，而输出则是描述输入的一行文本；或者使用手写的文稿作为输入，用自然的声音朗诵这串文本作为输出；或者，更简单的是，将一只金毛犬的图像与一个标志联系起来，告诉我们 \"是的，有一只金毛犬存在\"。这种能力使我们能够创建具有直到最近还是人类专有领域的功能的程序。 1.1 深度学习革命   为了领会这种深度学习方法所带来的范式转变，让我们回过头来看一看。直到过去十年，归于机器学习标签下的更广泛的一类系统都严重依赖于特征工程。特征是对输入数据的转换，便于下游算法（如分类器）在新数据上产生正确的结果。特征工程包括提出正确的变换，以便下游算法能够解决任务。例如，为了区分手写数字图像中的1和0，我们会想出一组滤波器来估计图像上的边缘方向，然后训练分类器来预测给定边缘方向分布的正确数字。另一个有用的特征可能是封闭的孔洞的数量，如在0、8，特别是环状2(loopy twos)中看到的。   另一方面，深度学习处理自动地从原始数据中找到这样的表征，以便成功地执行任务。在1对0的示例中，过滤器将在训练期间通过迭代查看成对的示例和目标标签来完善。这并不是说特征工程不适合深度学习，我们经常需要在学习系统中注入某种形式的先验知识。然而，神经网络通过摄取数据并根据示例提取有用的表征的能力是深度学习如此强大的原因。深度学习从业者的重点不是手工制作这些表征，而是对数学实体进行操作，使其自主地从训练数据中发现表征。通常，这些自动创建的特征比手工创建的特征更好！与许多颠覆性技术一样，这一事实导致了观点的改变。   在图1.1的左侧，我们看到从业者忙着定义工程特征，并将其反馈给学习算法；任务上的结果将和从业者工程上选择的特征一样好。在右侧，通过深度学习，将原始数据反馈给算法，在自身任务性能优化的引导下，自动提取层次化特征；任务的结果将和从业者推动算法朝着目标前进的能力一样好。 图1.1 深度学习交换了手工制作功能的需求，以满足数据和计算需求的增加 从图1.1的右侧开始，我们已经瞥见了执行成功的深度学习所需的条件： 我们需要一种方法来提取我们手头的任何数据。 我们需要以某种方式定义深度学习机器。 我们必须有一种自动化的方式，训练，以获得有用的表征，并使机器产生期望的输出。   这让我们不得不仔细考虑我们一直在说的训练这件事。在训练过程中，我们使用一个标准，即模型输出和参考数据的实值函数，为我们模型的期望输出和实际输出之间的差异提供一个数值分数（按照惯例，分数越低通常越好）。训练包括通过逐步修改我们的深度学习机器来推动标准向更低的分数发展，直到它达到低分，即使是在训练期间没有看到的数据上也是如此。 1.2 深度学习：PyTorch   PyTorch是一个Python程序库，有助于构建深度学习项目。它强调灵活性，允许采用习惯性的Python表达深度学习模型。这种亲和力和易用性在研究社区中找到了早期的采用者，自首次发布以来的几年中，它横跨广泛的应用领域，已经成长为最突出的深度学习工具之一。   正如Python对编程的作用一样，PyTorch提供了一个很好的深度学习入门。同时，事实证明，PyTorch完全有资格在专业背景下用于真实世界的高精尖工作。我们相信，PyTorch清晰的语法、精简的API和易于调试使其成为引进深度学习的绝佳选择。我们强烈建议学习PyTorch作为您的第一个深度学习库。至于它是否应该成为您学习的最后一个深度学习库，将由您自己决定。   事实上，图1.1中的深度学习机的核心是一个将输入映射到输出的相当复杂的数学函数。为了便于表达此函数，PyTorch提供了核心数据结构-张量，它是一个多维数组，与NumPy数组有许多相似之处。围绕这一基础，PyTorch提供了在专用硬件上执行加速数学运算的功能，这使得设计神经网络结构并在单机或并行计算资源上进行训练变得非常方便。   本书旨在为软件工程师、数据科学家和熟练掌握Python的学生提供一个起点，让他们能够自如地使用PyTorch来构建深度学习项目。我们希望本书尽可能地通俗易懂和实用，我们希望您能够将本书中的概念应用到其他领域。为此，我们采用了实践的方法，并鼓励您随时准备好您的计算机，这样您就可以使用这些示例，并将它们进一步推广。当我们读完本书的时候，我们希望您能够在优秀的官方文档的支持下，利用一个数据源，构建出一个深度学习项目。   尽管我们强调使用PyTorch构建深度学习系统的实用性，但我们相信，提供一个易懂的基础深度学习工具的介绍，不仅仅是为了促进新技术技能的获得，更是为了让来自广泛学科的新一代科学家、工程师和从业人员掌握工作知识，在未来几十年内成为许多软件项目的支柱。   为了充分利用这本书，您需要准备两样东西： 有一定的Python编程经验。我们不打算在这一点上做任何尝试；您需要掌握Python数据类型、类、浮点数字等。 愿意深入研究并动手操作。我们将从基础知识开始，积累我们的工作知识。如果您能跟着我们一起学习，您将会轻松很多。   《深度学习与PyTorch》分为三个不同的部分。第1部分涵盖了基础知识，详细研究了PyTorch提供的设施，以便通过代码将图1.1中的深度学习框架付诸实施。第2部分将引导您完成一个涉及医学成像的端到端项目：在CT扫描中查找肿瘤并对其进行分类，在第1部分介绍的基本概念基础构建，并添加更高级的主题。简短的第3部分以了解PyTorch为将深度学习模型部署到生产中所提供的功能来结束本书。   深度学习是一个巨大的领域。在本书中，我们将介绍该领域中的一小部分：具体来说，利用常见于图像处理的2D和3D数据集，采纳PyTorch实现较小范围的分类和分割项目。本书专注于实用的PyTorch，目的是涵盖足够多的领域，让您能够用深度学习解决现实世界的机器学习问题，例如在视觉领域，或者探索研究文献中出现的新模型。与深度学习研究相关的大部分（如果不是全部）最新出版物都可以在arXiV公共预印本库中找到，该库托管在 https://arxiv.org. 2 1.3 为什么使用深度学习   正如我们已经说过的，深度学习允许我们通过将我们的模型暴露在说明性的例子中，来执行非常广泛的复杂任务，比如机器翻译、玩策略游戏或识别混乱场景中的对象。为了在实践中做到这一点，我们需要的工具必须是灵活的，以便它们能够适应如此广泛的问题，并且要高效，可以在合理的时间内对大量数据进行训练；而且我们需要训练后的模型在输入存在可变性的情况下正确执行。让我们来看看我们决定使用PyTorch的一些原因。   PyTorch很容易被推荐，因为它非常简单。许多研究人员和从业人员发现它易于学习、使用、扩展和调试。它是Python式的，虽然像任何复杂的领域一样，它也有注意事项和最佳实践，但使用该库一般会让之前使用过Python的开发者感到熟悉。   更具体地说，在PyTorch中编写深度学习机是非常自然的。PyTorch为我们提供了一种数据类型，张量(Tensor)，通常用于保存数字、向量、矩阵或数组。此外，它还提供了对它们进行操作的函数。我们可以使用它们进行增量编程，如果需要，还可以进行交互式编程，就像我们在Python中习惯的那样。如果您了解NumPy，这将非常熟悉。   然而，PyTorch提供了两点使其与深度学习特别相关的东西：第一，它使用图形处理单元(GPU)提供加速计算，通常比在CPU上进行相同的计算提速50倍。其次，PyTorch提供了支持对通用数学表达式进行数值优化的设施，而深度学习将使用这些表达式进行训练。请注意，这两个功能一般都适用于科学计算，而不仅仅专用于深度学习。事实上，我们可以放心地将PyTorch定性为一个在Python中为科学计算提供优化支持的高性能库。   PyTorch的一个设计驱动因素是表现力，它允许开发人员实现复杂的模型，而不会被库强加过多的复杂性(它不是一个框架！)。PyTorch可以说是深度学习领域中最能将想法无缝翻译成Python代码的库之一。出于这个原因，PyTorch 在研究中得到了广泛的采用，这一点从国际会议上的高引用率就可以看出3。   PyTorch也有一段从研究和开发过渡到生产的令人信服的历史。虽然PyTorch最初专注于研究工作流，但它已经配备了高性能的C++ runtime，可以不依赖Python部署模型进行推理，并可用于用C++设计和训练模型。它还增加了与其他语言的绑定，以及用于部署到移动设备的接口。这些功能使我们能够利用PyTorch的灵活性，并将其引入到了难以获得完整的Python runtime或开销昂贵的应用程序中。   当然，声称易用性和高性能是微不足道的。我们希望，当您读完这本书的时候，您会同意我们的说法，我们在这里的说法是有根据的。 1.3.1 深度学习竞争格局   虽然所有的类比都是有缺陷的，但2017年1月PyTorch 0.1的发布似乎标志着深度学习库、包装器和数据交换格式从寒武纪爆炸式的扩散过渡到了一个整合和统一的时代。 注最近，深度学习领域的发展非常迅速，当您读到这篇印刷品的时候，它很可能已经过时了。如果您不熟悉这里提到的一些库，那也没关系。 在PyTorch的第一个测试版发布时： Theano和TensorFlow是首屈一指的底层库，它的工作模式是让用户定义一个计算图，然后执行它。 Lasagne和Keras是围绕Theano的高级封装器，Keras也封装了TensorFlow和CNTK Caffe、Chainer、DyNet、Torch（基于Lua的PyTorch的前身）、MXNet、CNTK、DL4J等填补了生态系统中的各种空缺。 在随后的大约两年时间里，格局发生了巨大的变化。社区基本上巩固了PyTorch或TensorFlow的地位，除了那些填补特定市场的库之外，其他库的采用率也在减少。简而言之： Theano，最早的深度学习框架之一，已经停止了主动开发。 TensorFlow： 完全融合了Keras，将其提升为一流的API 提供了一个即时执行的 \"eager mode\"，这与PyTorch处理计算的方式有些相似 发布了默认采用eager mode的TF 2.0 JAX，是Google开发的一个独立于TensorFlow的库，作为一个与NumPy对等的库，具有GPU、autograd和JIT功能，它已经开始获得吸引力。 PyTorch： 使用Caffe2作为后端 替换了大部分从基于Lua的Torch项目中重用的低级代码 增加了对ONNX的支持，ONNX是一种厂商中立的模型描述和交换格式 增加了一个延迟执行的 \"graph mode \"runtime，称为TorchScript 发布了1.0版本 取代了CNTK和Chainer，成为各自公司赞助商选择的框架 TensorFlow拥有强大的生产渠道、广泛的行业社区和巨大的市场份额。由于其易用性，PyTorch在研究和教学界取得了巨大的进展，自那以来，随着研究人员和毕业生培训学生并转向工业界，PyTorch的发展势头有所增强。在生产解决方案方面，它也积累了动力。有趣的是，随着TorchScript和eager mode的出现，PyTorch和TensorFlow的功能集都开始与对方的功能集趋于一致，尽管这些功能的呈现方式和整体体验仍然有很大的不同。 1.4 概述PyTorch如何支持深度学习项目   我们已经提到了PyTorch中的一些组件。现在，让我们花点时间来正式确定一下构成PyTorch的主要组件的高级地图。我们可以通过观察深度学习项目需要从PyTorch中获得什么来实现这一目标。   首先，PyTorch的\"Py\"和Python一样，但里面有很多非Python的代码。实际上，出于性能考虑，PyTorch的大部分代码是用C++和CUDA (https://www.geforce.com/hardware/technology/cuda) 编写的，CUDA是NVIDIA的一种类似于C++的语言，经过编译后可以在GPU上大规模并行运行。有一些方法可以直接从C++中运行PyTorch，我们将在第15章中研究这些方法。这种能力的动机之一是为在生产中部署模型提供可靠的策略。然而，大多数时候，我们会从Python中与PyTorch进行交互，构建模型、训练模型，并使用训练后的模型来解决实际问题。   事实上，Python API是PyTorch在可用性和与更广泛的Python生态系统集成方面的亮点。让我们来看看PyTorch的心理模型。   正如我们已经提到的，PyTorch的核心是一个提供多维数组的库，也就是 PyTorch的术语中的 tensors（张量，我们将在第3章中详细介绍），以及由 torch模块提供的关于它们的大量操作库。tensors 和对它们的操作都可以在 CPU 或 GPU 上使用。在PyTorch中，将计算从CPU转移到GPU上，所需要的不过是一两个额外的函数调用。PyTorch 提供的第二项核心功能是，能够让tensors跟踪对其执行的操作，并分析计算输出相对于任何输入的导数。这是用于数值优化的，而且是由 tensors通过PyTorch库下的 autograd 引擎调度而原生提供的。   通过拥有张量和支持自动梯度的张量标准库，PyTorch可以用于物理、渲染、优化、模拟、建模等-我们很有可能在整个科学应用领域看到PyTorch以创造性的方式被使用。但PyTorch首先是一个深度学习库，因此它提供了构建和训练神经网络所需的所有构件。图1.2显示了加载数据、训练模型，然后将该模型部署到生产环境中的标准设置。   构建神经网络的核心PyTorch模块位于torch.nn中，它提供了常见的神经网络层和其他架构组件。全连接层、卷积层、激活函数和损失函数都可以在这里找到（我们将在本书的其他部分详细介绍这些内容的含义）。这些组件可以用来建立和初始化我们在图1.2中心看到的未经训练的模型。为了训练我们的模型，我们需要一些额外的东西：训练数据源、使模型适应训练数据的优化器，以及将模型和数据送到实际执行训练模型所需计算的硬件的方法。 图1.2 PyTorch项目的基本、高级结构，包括数据加载、培训和部署到生产   在图1.2左侧，我们看到，在训练数据到达我们的模型之前，需要进行相当多的数据处理4。首先，物理获取数据，通常是从某种类型的存储中获取数据作为数据源。然后，我们需要将数据中的每个样本转换为PyTorch能够实际处理的东西：tensors(张量)。在我们的自定义数据（无论其格式如何）和标准化的 PyTorch 张量之间的桥梁就是 torch.utils.data 中 PyTorch 提供的 Dataset 类。由于这个过程在不同的问题上有很大的差异，所以我们必须自己实现这个数据源。我们将在第4章中详细研究如何将我们可能要处理的各种类型的数据表示为张量。   由于数据存储往往很慢，特别是由于访问延迟，我们希望能实现数据加载的并行化。但由于Python诸多优点并不包括简单、高效、并行处理，因此我们将需要多个进程来加载我们的数据，以便将它们组装成batches批处理：包含多个样本的张量。这相当复杂；但由于它也比较通用，PyTorch在DataLoader类中很容易地提供了所有这些。它的实例可以派生生子进程，以便在后台从数据集中加载数据，这样，只要训练循环能够使用它，它就可以准备好并等待训练循环。我们将在第7章认识和使用Dataset和DataLoader。   有了获取批量样本的机制，我们可以转向图1.2中心的训练循环本身。通常情况下，训练循环是作为一个标准的Python for循环来实现的。在最简单的情况下，模型在本地CPU或单个GPU上运行所需的计算，一旦训练循环有了数据，计算就可以立即开始。这有可能是你的基本设置，这也是我们在本书中假设的设置。   在训练循环的每一步，我们都会根据从数据加载器获得的样本来评估我们的模型。然后，我们使用某种标准或损失函数将模型的输出与预期输出（目标）进行比较。正如它提供了用于构建模型的组件一样，PyTorch也有各种损失函数供我们使用。它们也在 torch.nn 中提供。在将我们的实际输出与具有损失函数的理想输出进行比较之后，我们需要稍微改动模型，使其输出更接近目标值。如前所述，这就是 PyTorch autograd 引擎的作用；但我们还需要一个优化器来进行更新，这就是 PyTorch 在 torch.optim 中为我们提供的功能。我们将在第5章中开始研究使用损失函数和优化器的训练循环，然后在第6章到第8章中磨练我们的技能，最后再着手进行第2部分的大项目。   使用更复杂的硬件越来越常见，比如多个GPU或多台机器为训练一个大型模型共同贡献自己的资源，如图1.2底部中心所示。在这些情况下，可以采用 torch.nn.parallel.Distributed-DataParallel 和 torch.distributed 子模块来使用额外的硬件。   训练循环可能是深度学习项目中最乏味却最耗时的部分。在它结束时，我们得到的回报是一个模型，它的参数已经在我们的任务上得到了优化：图中训练循环右边描绘的训练模型。有一个模型来解决任务是很好的，但为了让它有用，我们必须把它放在需要工作的地方。这个过程中的部署部分，如图1.2中右侧描绘，可能涉及将模型放在服务器上，或者导出模型加载到云引擎。或者我们可能会将其与一个更大的应用程序集成，或者在手机上运行。   部署实践的一个特定步骤可以是导出模型。如前所述，PyTorch默认为立即执行模型（EAGER模式）。每当涉及PyTorch的指令被Python解释器执行时，相应的操作就会被底层的C++或CUDA实现立即执行。随着对 tensors 进行操作的指令增多，后端实现会执行更多的操作。 PyTorch还提供了一种通过TorchScript提前编译模型的方法。 使用TorchScript，PyTorch可以将模型序列化为一组指令，这些指令可以独立于Python调用：比方说，从C++程序或在移动设备上调用。 我们可以将其视为具有特定于张量操作的有限指令集的虚拟机。 这允许我们将模型导出为与PyTorch运行时一起使用的TorchScript，或者以称为ONNX的标准化格式导出。 这些功能是PyTorch生产部署功能的基础。 我们将在第15章中讨论这一点。 PyTorch 还提供了一种通过 TorchScript 提前编译模型的方法。使用TorchScript，PyTorch可以将模型序列化为一套独立于Python调用的指令：比如说，从C++程序或移动设备上调用。我们可以将其视为具有特定于张量操作的有限指令集的虚拟机。这使得我们可以将模型导出，或者作为TorchScript与PyTorch runtime一起使用，或者以名为ONNX的标准化格式导出。这些功能是 PyTorch 生产部署能力的基础。我们将在第 15 章中介绍这一点。 1.5 硬件和软件要求   本书需要对涉及大量数值计算的任务进行编程和运行，如大量矩阵的乘法。事实证明，在新的数据上运行一个预先训练好的网络，是任何一台最新的笔记本电脑或个人计算机都能做到的。即使是采用预训练的网络，并对其中的一小部分进行再训练，使其在新的数据集上专业化，也不一定需要专门的硬件。你可以使用标准的个人计算机或笔记本电脑来跟进我们在本书第1部分所做的一切。   然而，我们预计，完成第2部分中更高级的示例的完整训练运行将需要一个支持CUDA的GPU。第2部分中使用的默认参数假设GPU的内存为8 GB（我们建议使用NVIDIA GTX 1070或更高版本），但如果你的硬件可用内存较少，这些参数可以调整。需要说明的是：如果你愿意等待，这样的硬件并不是必须的，但在GPU上运行至少可以将训练时间缩短一个数量级（通常会快40-50倍）。单独来看，计算参数更新所需的操作在现代硬件上（从几分之一秒到几秒）是很快的，比如典型的笔记本电脑CPU。问题在于，训练涉及反复、多次地运行这些操，增量更新网络参数，以最小化训练误差。   中等规模的网络在配备好GPU的工作站上从头开始训练大型真实世界的数据集可能需要数小时到数天的时间。通过在同一台机器上使用多个GPU，甚至在配备多个GPU的机器集群使用多个GPU，可以进一步缩短这一时间。多亏了云计算提供商提供的服务，这些配置并不像听起来那样令人望而却步。DAWNBench (https://dawn.cs.stanford.edu/benchmark/index.html) 是斯坦福大学的一个有趣的计划，旨在提供与公开可用数据集上的常见深度学习任务相关的训练时间和云计算成本的基准。   因此，如果你在阅读至第2部分时，已经拥一个GPU，那就太好了。否则，我们建议查看各种云平台的产品，其中许多平台提供了支持GPU的Jupyter Notebooks，并预装了PyTorch，通常是有免费配额的。Google Colaboratory (https://colab.research.google.com) 是一个很好的开始。   最后一个考虑事项是操作系统(OS)。PyTorch从第一个版本开始就支持Linux和MacOS，并在2018年获得了Windows的支持。由于目前的苹果笔记本电脑不包括支持CUDA的GPU，因此PyTorch的预编译MacOS软件包仅适用于CPU。在整本书中，我们将尽量避免假设您正在运行特定的操作系统，尽管第2部分中的一些脚本似乎是从Linux下的Bash命令提示行运行的。这些脚本的命令行应该很容易转换为与Windows兼容的形式。为方便起见，在可能的情况下，代码将按照从Jupyter Notebook中运行的方式列出。   有关安装信息，请参见PyTorch官方网站 (https://pytorch.org/get-started/locally) 上的入门指南。我们建议Windows用户使用Anaconda或Miniconda (https://www.anaconda.com/distribution 或 https://docs.conda.io/en/latest/miniconda.html) 进行安装。其他操作系统(如Linux)通常具有更广泛的可用选项，Pip 是Python最常见的包管理器。我们提供了一个requirements.txt文件，pip可以使用该文件安装依赖项。当然，有经验的用户可以自由地以与您首选的开发环境最兼容的方式安装软件包。   第2部分也有一些不小的下载带宽和磁盘空间要求。第2部分中癌症检测项目所需的原始数据下载量约为60 GB，解压后需要约120 GB的空间。压缩后的数据可以在解压后删除。此外，出于性能考虑，要缓存部分数据，在训练时还需要80GB。你将需要在系统上准备总共200 GB（至少）的可用磁盘空间，用于训练。虽然可以使用网络存储，但如果网络访问速度慢于本地磁盘，则可能会有训练速度损失。你最好在本地SSD上有空间来存储数据，以便快速检索。 1.5.1 使用Jupyter Notebooks   我们将假设您已经安装了PyTorch和其他依赖项，并且已经验证了一切正常。前面我们提到了遵循本书中的代码的可能性。我们将在示例代码中大量使用Jupyter Notebooks。Jupyter Notebook在浏览器中显示为一个页面，我们可以通过它交互式地运行代码。代码由一个kernel（内核）进行评估，该kernel是一个运行在服务器上的进程，它随时准备接收代码执行并将结果发回，然后在页面上内联渲染。notebook将内核的状态和代码评估过程中定义的变量一样，维护在内存中，直到它被终止或重新启动。我们与notebook交互的基本单位是一个cell（单元格）：页面上的一个方框，我们可以在其中键入代码并让内核对其求值(通过菜单项或按Shift-Enter)。我们可以在一个notebook中添加多个cell，新的cell将可见我们在前面的cell中创建的变量。cell的最后一行返回的值在执行后会被打印在cell的正下方，绘图也是如此。通过混合源代码、评估结果和Markdown格式的文本单元格，我们可以生成漂亮的交互式文档。你可以在项目网站 (https://jupyter.org) 上阅读关于Jupyter Notebooks的所有内容。   此时，您需要从GitHub的代码签出的根目录启动notebook服务器。具体如何启动服务器取决于你的操作系统的细节，以及你安装Jupyter的方式和位置。如果您有任何问题，欢迎在本书的论坛上提问。5启动后，您的默认浏览器将弹出，显示本地笔记本文件的列表。   这时，你需要从GitHub的代码签出的根目录下启动笔记本服务器。具体如何启动服务器取决于你的操作系统的细节，以及你安装Jupyter的方式和地点。如果你有问题，欢迎在本书的论坛上提问5。启动后，你的默认浏览器会弹出，显示本地笔记本文件的列表。 注 Jupyter Notebooks是通过代码表达和研究想法的强大工具。虽然我们认为它们非常适合我们这本书的用例，但它们并不适合每个人。我们认为,专注于消除摩擦和最小化认知开销是很重要的，这对每个人来说都是不同的。在使用PyTorch进行试验期间，你可以随意使用。   书中所有列表的完整工作代码可以在本书的网站 (https://www.manning.com/books/deep-learning-with-pytorch) 和我们在GitHub上的存储库 (https://github.com/deep-learning-with-pytorch/dlwpt-code) 中找到。 1.6 练习 启动Python，以获得交互式提示窗口。 您使用的是什么Python版本？我们希望至少是3.6！ 您能import torch吗？您得到的是什么版本的PyTorch？ torch.cuda.is_available()的结果是什么？根据您使用的硬件，它是否符合您的预期？ 启动Jupyter notebook server。 Jupyter使用的是什么版本的Python？ Jupyter使用的torch库的位置与您从交互提示符导入的位置相同吗？ 1.7 小结 深度学习模型会自动学习从实例中关联输入和期望的输出。 像PyTorch这样的库可以让您高效地构建和训练神经网络模型。 PyTorch最大限度地减少了认知开销，同时注重灵活性和速度。它还默认为立即执行操作。 TorchScript允许我们预编译模型，不仅可以从Python中调用它们，还可以从C++程序和移动设备上调用它们。 自2017年初发布PyTorch以来，深度学习工具生态系统已经明显巩固。 PyTorch提供了许多实用程序库来促进深度学习项目。 1 Edsger W. Dijkstra, “The Threats to Computing Science,” http://mng.bz/nPJ5. 2 我们还推荐www.arxiv-sanity.com帮助组织感兴趣的研究论文。 3 在2019年国际学习表征会议（ICLR）上，PyTorch被252篇论文引用，高于上一年的87篇，与TensorFlow出现在266篇论文中的水平相同。 5 这只是即时完成的数据准备，而不是预处理，这在实际项目中可能是相当大的一部分。 5 https://forums.manning.com/forums/deep-learning-with-pytorch By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "},"Chapter2/2.1.html":{"url":"Chapter2/2.1.html","title":"2. 预训练网络","keywords":"","body":"2. 预训练网络 本章包括：  - 运行预训练的图像识别模型  - GANs和CycleGAN的介绍  - 可以产生图像文字描述的字幕模型  - 通过Torch Hub共享模型 我们在第一章结尾处承诺，将在这一章中揭开令人惊奇的事物，现在是时候兑现了。出于多种原因，计算机视觉无疑是受深度学习出现影响最大的领域之一。存在着对自然图像内容进行分类或解释的需求，非常大的数据集变得可用，卷积层等新的构造被发明出来，并且可以可以在GPU上以前所未有的精度快速运行。所有这些因素与互联网巨头们渴望了解由数百万用户使用其移动设备拍摄并在所述巨头平台上进行管理的照片这一事实相结合。一场完美的风暴终于降临。 我们将通过下载和运行非常有趣的模型来学习如何利用该领域最好的研究人员的工作，这些模型已经在开放的、大规模的数据集上进行了培训。我们可以把预训练的神经网络想象成类似于一个接受输入并产生输出的程序。这样的程序的行为由神经网络的体系结构以及在训练过程中看到的示例来决定，它们取决于所需的输入-输出对或输出应满足的所需属性。使用现成的模型可能是一种启动深度学习项目的快速方式，因为它借鉴了设计模型的研究人员的专业知识，以及训练权重所需的计算时间。 在这一章中，我们将探索三种流行的预训练模型：一种可以根据图像的内容对其进行标记的模型，另一种可以从真实图像生成新图像的模型，以及一个可以使用合适的英语句子描述图像内容的模型。 我们将学习如何在PyTorch中加载和运行这些预先训练模型，并且将介绍PyTorch Hub，这是一组工具，通过它们，可以通过统一的接口轻松地使用像我们将要讨论的那些模型。在此过程中，我们将讨论数据源，定义标签等术语，并参加一场“斑马竞技”。 如果您是从其他深度学习框架来学习PyTorch的，而且您更愿意直接跳到PyTorch的核心内容，则可以跳到下一章。本章中我们将涉及的内容更多的是趣味性而非基础性的，并且在一定程度上独立于任何特定的深度学习工具。这并不是说它们不重要! 但如果你已经在其他深度学习框架中使用过预训练的模型，那么你已经知道它们是多么强大的工具。而且，倘若你已经熟悉了生成式对抗网络（GAN）游戏，那你就更不需要我们向你解释了。 不过，我们希望你能继续阅读，因为本章在乐趣之下隐藏了一些重要的技能。学习如何使用 PyTorch 运行预训练的模型是一项有用的技能--这点完全没错。如果模型是在一个大型数据集上训练出来的，则特别有用。我们需要习惯于在真实世界的数据上获取和运行神经网络的机制，然后对其输出进行可视化和评估，无论我们是否对其进行了训练。 2.1 一种可识别图像主体的预训练网络 作为我们深度学习的首次尝试，我们将运行一个最先进的深度神经网络，该网络经过了对象识别任务的预先训练。有许多预训练的网络可以通过源代码存储库访问到。研究人员在发表论文的同时发布他们的源代码是很常见的，通常代码中会附带有通过在参考数据集上训练模型获得的权重。例如，使用其中一种模型可以让我们轻松地为下一个Web服务配备图像识别功能。 我们在这里要探讨的预训练网络是在ImageNet数据集的一个子集上训练得到的 (http://imagenet.stanford.edu) 。ImageNet是由斯坦福大学维护的包含1400多万张图像的庞大的数据集。所有图像都用来自wordnet数据集 (http://wordnet.princeton.edu) 的名词层次结构进行了标记，而该数据集又是一个大型的英语词汇数据库。 ImageNet数据集与其他一些公共数据集一样，起源于学术竞赛。竞赛历来是一些机构和公司的研究人员经常相互挑战的主要赛场。其中，ImageNet大规模视觉识别挑战赛（ILSVRC）自2010年创办以来就广受欢迎。这项特殊的比赛基于多项任务，每年的任务内容可能会有所不同，如图像分类（告诉图像中包含哪些对象类别）、对象定位（识别对象在图像中的位置）、对象检测（识别和标记图像中的对象）、场景分类（对图像中的情况进行分类）和场景解析（将图像分割成与语义类别相关的区域，例如奶牛、房子、奶酪、帽子）。具体来说，图像分类任务包括获取一张输入图像，并从1000个总类别中产生5个标签列表，按置信度排序，描述图像的内容。 ILSVRC的训练集由120万张图像组成，这些图像用1000个名词中之一进行标注（例如 \"dog\"），称为图像的类别。在这个意义上讲，我们将交替使用label(标签) 和clase(类) 这两个术语。我们可以从图2.1中查看一下ImageNet中的图像。 图2.1 ImageNet中的部分图片样本 还有， 图2.2 推断过程 我们最终要能够把我们自己的图像输入到我们的预训练模型中，如图2.2所示。这会产生该图像的预测标签列表，然后我们可以检查该列表以查看模型认为我们的图像是什么。有些图像的预测很准确，而有些图像则不会。 输入的图像将首先被预处理成一个多维数组类 torch.Tensor 的实例。它是一个有高度和宽度的RGB图像，因此该张量将具有三个维度：三个颜色通道与两个特定尺寸的空间图像维度。(我们将在第3章中详细介绍什么是张量，但现在，将其视为像浮点数字的向量或矩阵。) 我们的模型将把预处理后的输入图像传入预训练网络，以获得每个类的分数。根据权重，最高的分数对应着最有可能的类。然后，每个类会被一对一地映射到一个类标签上。该输出包含在一个有1000个元素的torch.Tensor中，每个元素代表与该类相关的分数。 在完成所有操作之前，我们需要得到网络本身，深入了解一下它的结构，并掌握如何在使用模型之前准备好数据。 2.1.1 获取用于图像识别的预训练网络 如上所述，我们现在将为自己配备一个在ImageNet上训练好的网络。为此，我们将看看TorchVision项目 (https://github.com/pytorch/vision) ，它包含了一些性能最好的计算机视觉神经网络架构，如AlexNet (http://mng.bz/lo6z) 、ResNet (https://arxiv.org/pdf/1512.03385.pdf) 和Inception v3 (https://arxiv.org/pdf/1512.00567.pdf) 。它还可以轻松访问ImageNet等数据集和其他实用程序，以便在PyTorch中加快计算机视觉应用的速度。我们将在本书中进一步探讨其中一些内容。 现在，让我们加载并运行两个网络：首先是AlexNet，这是图像识别的早期突破性网络之一；然后是残差网络，简称ResNet，它在2015年赢得了ImageNet分类、检测和定位等比赛。如果你没有在第1章中启动并运行过PyTorch，那么现在是进行此操作的好时机。 预训练的模型可以在 torchvision.models 中找到(code/p1ch2/2 _pre_trained_networks.ipynb) 如： # In[1]: from torchvision import models 我们可以看看实际的模型： # In[2]: dir(models) # Out[2]: ['AlexNet', 'DenseNet', 'Inception3', 'ResNet', 'SqueezeNet', 'VGG', ... 'alexnet', 'densenet', 'densenet121', ... 'resnet', 'resnet101', 'resnet152', ... ] 这些大写的名称指的是实现了许多流行模型的 Python 类。它们在架构上有所不同 -- 也就是说，在输入和输出之间的操作安排上有所不同。小写的名字是便捷函数(convenience functions)，它们返回从这些类实例化的模型，有时带有不同的参数集。例如，resnet101返回具有101层的ResNet实例，resnet18有18层，等等。现在，我们将注意力转向AlexNet。 2.1.2 AlexNet AlexNet架构在2012年ILSVRC上大获全胜，取得了前5项测试错误率（正确的标签必须在前5个预测中）为15.4％的好成绩。相比之下，提交的第二名并不是基于深度网络的，落后于26.2%。这是计算机视觉历史上的一个决定性时刻：社区开始意识到深度学习在视觉任务中的潜力。在实现这一飞跃之后，以更多的现代体系结构和训练方法不断地进行了改进，使前5位错误率降低至3％。 按照今天的标准，与最先进的模型相比，AlexNet只是一个相当小的网络。但在我们的案例中，它是初探神经网络的完美之选，也将被用来学习如何在新图像上运行它的预训练版本。 我们可以从图2.3中看到AlexNet的结构。并不是说我们现在已经具备了理解它的所有要素，但我们可以预见几个方面。首先，每个块都由一堆乘法和加法组成，再加上我们将在第5章了解的输出中的少量其他函数。我们可以把它看成是一个滤波器--一个把单个或多个图像作为输入并产生其他图像作为输出的函数。在训练过程中，它将根据所看到的示例以及这些示例的期望输出来确定其执行方式。 图2.3 AlexNet结构 在图2.3中，输入图像从左侧进入，并通过五个滤波器，每个过滤器都会产生多个输出图像。在每一次滤波后，图像的尺寸都会缩小，如图所示。由最后一组滤波器产生的图像被设置为一个4096个元素的一维矢量，并被分类以产生1000种输出概率，每个输出类别各一个。 为了在输入图像上运行AlexNet架构，我们可以创建AlexNet类的实例。它是这样实现的： # In[3]: alexnet = models.AlexNet() 此处，alexnet是一个可以运行AlexNet架构的对象。我们暂时没有必要去了解这个架构的细节目前，AlexNet只是一个可以像函数一样调用的不透明对象。通过向alexnet提供一些特定尺寸的输入数据（我们很快就会看到这类输入数据应该是什么），我们将通过网络运行一次forward pass(正向传递)。也就是说，输入将通过第一组神经元，其输出将被馈送给下一组神经元，直到最终的输出。实际上，假设我们有一个正确类型的输入对象，我们可以用output = alexnet(input)来进行正向传递。 但是，如果这样做，我们将通过整个网络馈送数据以产生...垃圾！ 这是因为网络尚未得到初始化：它的权重，也就是输入相乘和相加的数字尚未经过任何训练-网络本身是一片空白（或者说是随机的）。我们需要从头开始对其进行训练，或者从以前的训练中加载权重，现在我们将进行此操作。 为此，让我们回到models模块。我们了解到，大写名称对应于实现计算机视觉体系结构的流行的类。另一方面，小写名称是用预定数量的层和单位实例化模型，并可以选择将预训练的权重下载并加载到其中的函数。 请注意，其实并没有必要使用这些函数：它们只是使您可以方便地使用与预训练网络的构建方式匹配的多个层和单元来实例化模型。 2.1.3 ResNet 长远来看，在2015年残差网络出现之前，实现如此深度的稳定训练被认为是极其困难的。残差网络采用了一定的技巧，让它成为了可能，并借此一举打破了当年的几项基准。 现在让我们创建一个网络实例。我们将传递一个参数，指示函数下载在ImageNet数据集上训练的 resnet101的权重，其中包括120万张图片和1000个类别： # In[4]: resnet = models.resnet101(pretrained=True) 当我们盯着下载进度时，可以花一点时间欣赏一下resnet101的4450万个参数-其中有许多参数可以自动优化！ 2.1.4 准备，设定，即将运行 好，我们刚刚得到什么？ 既然我们很好奇，我们就来看一下resnet101是什么样子的。我们可以通过打印返回模型的值来做到这一点。这为我们提供了2.3中看到的相同类型信息的文本表示，并提供了有关网络结构的详细信息。目前位置，信息量可能有点大，但是随着本书的进展，我们将提高理解此代码内涵的能力： # In[5]: resnet # Out[5]: ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( ... ) ) (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0) (fc): Linear(in_features=2048, out_features=1000, bias=True) ) 我们在这里看到的是modules，每行一个。请注意，它们与Python模块没有任何共同之处：它们是单个操作，是神经网络的构建块。它们在其他深度学习框架中也被称为layers(层)。 如果我们向下滚动，我们会看到很多Bottleneck模块一个接一个地重复（101个！），其中包含卷积和其他模块。这就是一个典型的计算机视觉深度神经网络的解剖结构：一个或多或少有顺序的滤波器和非线性函数的级联，最终以层（fc）为1000个输出类（out_features）中的每一个产生对应分数而结束。 resnet变量可以像函数一样被调用，将一张或多张图像作为输入，并为1,000个ImageNet类中的每一个产生相同数量的分数。可以像调用函数一样调用resnet变量，它接受一张或多张图像作为输入，并为1000个ImageNet类中的每个类生成相同数量的分数。然而，在执行此操作之前，我们必须对输入图像进行预处理，使它们的大小合适，并且使它们的值（颜色）大致处于相同的数值范围内。为了做到这一点，torchvision模块提供了变换，它允许我们快速定义基本预处理函数的管道： # In[6]: from torchvision import transforms preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] )]) 在本例中，我们定义了一个预处理函数，该函数会把输入图像缩放为256×256，在中心位置将图像裁剪为 24×224，将其转换为张量（PyTorch多维数组：在本例中，是一个具有颜色、高度和宽度的3D数组），并对其 RGB（红、绿、蓝）分量进行归一化处理，使它们具有定义的平均值和标准差。如果我们希望网络产生有意义的答案，这些需要与训练期间呈现给网络的内容相匹配。当我们在7.1.3节深入研究制作我们自己的图像识别模型时，我们将更深入地了解变换。 现在我们可以抓取一张我们最喜欢的狗的图片（比如说，GitHub repo中的bobby.jpg），对其进行预处理，然后看看ResNet对它的判断。我们可以先用Pillow (https://pillow.readthedocs.io/en/stable) 从本地文件系统加载一张图片，Pillow是Python的一个图像处理模块： # In[7]: from PIL import Image img = Image.open(\"../data/p1ch2/bobby.jpg\") 如果我们从Jupyter Notebook进行跟踪，则可以执行以下操作来查看内联图片(它将显示在中)： # In[8]: img # Out[8]: 或者，我们可以调用show方法，弹出一个带有查看器的窗口，就可以看到图2.4所示的图像： 图2.4 狗狗，我们特别输入的图片 然后， >>> img.show() 接下来，我们可以通过预处理管道传递图像： # In[9]: img_t = preprocess(img) 然后，我们可以以网络期望的方式对输入张量进行重塑、裁剪和归一化。我们将在接下来的两章中了解更多的内容，现在先稍等片刻。 # In[10]: import torch batch_t = torch.unsqueeze(img_t, 0) 我们现在准备好运行我们的模型了。 2.1.5 运行 在新的数据上运行训练好的模型的过程，在深度学习界称为inference(推理)。为了进行推理，我们需要将网络置于eval模式: # In[11]: resnet.eval() # Out[11]: ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( ... ) ) (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0) (fc): Linear(in_features=2048, out_features=1000, bias=True) ) 如果我们忘记了这样做，一些预先训练的模型，如batch normalization(批归一化) 和dropout(随机失活)，因为它们内部的工作方式,将不会产生有意义的答案。既然已经设置了eval，我们就可以进行推理了： # In[12]: out = resnet(batch_t) out # Out[12]: tensor([[ -3.4803, -1.6618, -2.4515, -3.2662, -3.2466, -1.3611, -2.0465, -2.5112, -1.3043, -2.8900, -1.6862, -1.3055, ... 2.8674, -3.7442, 1.5085, -3.2500, -2.4894, -0.3354, 0.1286, -1.1355, 3.3969, 4.4584]]) 刚刚发生了一组涉及4450万个参数的惊人操作，每个ImageNet类产生1000个分数的向量。这并没有花费太多时间，是吗？ 我们现在需要找出获得最高分的类的标签。这将告诉我们模型在图像中看到了什么。如果标签与人类描述图像的方式相吻合，那就太棒了！这意味着一切正常。如果没有，要么是在训练过程中出了问题，要么是图像与模型预期的差异太大，以至于模型无法正确处理它，或者存在其他类似的问题。 为了查看预测标签的列表，我们将加载一个文本文件，按照训练时呈现给网络的相同顺序列出标签，然后我们将在index(索引) 处挑出网络产生最高分的标签。几乎所有用于图像识别的模型，其输出的形式都与我们即将使用的类似。 让我们加载包含ImageNet数据集类的1000个标签的文件： # In[13]: with open('../data/p1ch2/imagenet_classes.txt') as f: labels = [line.strip() for line in f.readlines()] 这时，我们需要确定之前得到的out张量中最大分值所对应的索引。我们可以使用PyTorch中的max函数来完成，它可以输出张量中的最大值以及该最大值的索引： # In[14]: _, index = torch.max(out, 1) 现在,我们可以使用索引来访问标签。这里，index并不是一个普通的Python数字，而是一个一元，一维的张量(具体来说是tensor([207])，所以我们需要使用index[0]获取实际的数值作为索引进入我们的标签列表。我们还使用 torch.nn.functional.softmax (http://mng.bz/BYnq) 将我们的输出归一化到 [0, 1] 范围内，然后除以总和。这使我们大致了解该模型对其预测的信心。在这种情况下，模型有96%的把握确定它所看到的是一只金毛犬： # In[15]: percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100 labels[index[0]], percentage[index[0]].item() # Out[15]: ('golden retriever', 96.29334259033203) 喔，真是一条好狗狗~ 由于模型产生了分数，我们还可以找出第二可能性、第三可能性等。为此，我们可以使用sort函数，将数值按升序或降序进行排序，同时提供排序后的数值在原始数组中的索引： # In[16]: _, indices = torch.sort(out, descending=True) [(labels[idx], percentage[idx].item()) for idx in indices[0][:5]] # Out[16]: [('golden retriever', 96.29334259033203), ('Labrador retriever', 2.80812406539917), ('cocker spaniel, English cocker spaniel, cocker', 0.28267428278923035), ('redbone', 0.2086310237646103), ('tennis ball', 0.11621569097042084)] 我们看到前四个都是狗(谁能想到redbone是一个品种呢？)，之后事情就开始变得有趣了。第五个答案是\"网球\"，可能是因为附近有足够多的带狗的网球图片，模型本质上是在说：\"有0.1%的可能性，我完全误解了网球是什么。\" 这是一个很好的例子，说明了人类和神经网络看待世界的根本差异，也说明了奇怪的、微妙的偏差是多么容易潜入我们的数据。 实操时间到了! 我们可以继续用随机图像来测试我们的网络，并查看其结果。网络的成功程度很大程度上取决于训练集中的主体是否得到了很好的代表。如果我们提出一张包含训练集之外的主体的图像，网络很有可能会得出一个置信度相当高的错误答案。实验一下，感受一下模型对未曾见过的数据的反应是很有用的。 我们刚刚运行了一张网络，该网络在2015年赢得了图像分类比赛。它学会了从狗和大量其他真实世界的对象的示例中识别出了我们的狗。从图像生成开始，我们现在将了解不同的架构是如何实现其他类型的任务的。 2.2 一种从伪到真的预训练模型 让我们暂时假设一下，我们是职业罪犯，想倒卖出售著名艺术家“丢失”画作的赝品。考虑到我们是罪犯，而不是画家，所以当我们画出伦勃朗和毕加索的赝品时，很快就会被发现它们是业余的仿制品，而不是真品。即使我们花了很多时间练习，最终得到一张我们无法分辨真伪的画作，但如果想在当地的艺术品拍卖行冒充行骗的时候，我们仍旧会被立刻扫地出门。更糟糕的是，被批判“这是冒牌货，滚粗”并不能帮助我们改进。我们将不得不随机尝试一大堆东西，判断哪些需要稍微长一点的时间才能被识别为赝品，并在我们未来的尝试中增强这些特征，而这将花费太多的时间。 相反，我们可以找一位缺德的艺术史学家来检查我们的作品，并准确告诉我们这幅画看起来不对劲的原因。有了这些反馈，我们就可以用清晰、直接的方式来改进我们的作品，直到我们的粗略学者无法再分辨出我们的画作和真迹的区别。 很快，我们的“Botticelli”(译者注：波提切利，佛罗伦萨画派著名画家)就会出现在卢浮宫，而他们的“Benjamins”(译者注：本杰明·富兰克林，100美元上印刷的人物)就会变到我们的口袋里。一夜暴富! 虽然这种情况有点滑稽，但潜在的基础技术是可靠的，并且可能会在未来几年对数字数据的真实性认知产生深远影响。鉴于自动生成具有说服力的伪图像和伪视频的过程将非常容易，\"照片证据\"的整个概念很可能变得不那么可靠。唯一的关键因素是数据。让我们看看这个过程是如何运作的。 2.2.1 生成式对抗网络问题(The GAN game) 在深度学习的背景下，我们刚才描述的是GAN game，在这场博弈中，两个网络（一个充当绘画者，另一个充当艺术史学家）在创造和检测伪造方面相互竞争，以期胜过彼此。 GAN代表生成式对抗网络(generative adversarial network)，生成式(generative) 意味着正在创造某种东西(在本例中，是伪造的名作)，对抗性(adversarial) 意味着两个网络在竞争，试图胜过对方，至于网络(network)，显而易见。这些网络是最近深度学习研究中最具原创性的成果之一。 请记住，我们的首要目标是生成一类不能被识别为假的图像的合成示例。当与正常的示例混到一起时，熟练的审查员也将很难判断哪些是真的，哪些是我们的赝品。 生成器网络在我们的案例中充当画家的角色，任务是从任意输入开始生成逼真的图像。 判别器网络则是不道德的艺术检查员，需要判断给定的图像是由生成器伪造的还是属于一组真实的图像。对于大多数深度学习架构而言，这种由两个网络组成的设计都是非典型的，但当用于实现GAN game时，可能会产生令人难以置信的结果。 图2.5 GAN game的概念 图2.5显示了正在发生的事情的大致流程。生成器的最终目标是欺骗判别器，使其混淆真假图像。判别器的最终目标是找出它是何时被欺骗的，但它也有助于告知生成器生成图像中的可识别错误。一开始，生成器生成的是混乱的三眼怪物，看起来一点也不像伦勃朗的肖像。判别器可以很容易地将混乱的画与真实的画区分开来。在训练进行时，信息从判别器回流，生成器利用这些信息进行改进。等到训练结束时，生成器能够产生令人信服的赝品，判别器不再能区分哪个是哪个。 请注意，我们不应该从字面上理解\"判别器获胜\"或 \"生成器获胜\"--两者之间并没有明确的竞赛。然而，这两个网络都是基于对方的结果进行训练的，这推动了每个网络参数的优化。 这项技术已经证明，它本身能够通过噪声和条件信号训练出能生成逼真的图像的生成器，比如属性(例如，针对人脸：年轻、女性、戴眼镜)或另一幅图像。换句话说，训练有素的生成器可以学习一种合理的模型，生成即使在人类检查时也看起来很真实的图像。 2.2.2 CycleGAN 这个概念的一个有趣的演变是CycleGAN。CycleGAN可以将一个领域的图像转化为另一个领域的图像（然后再返回），而不需要我们在训练集中显式地提供匹配对。 在图2.6中，我们展示了一个CycleGAN的工作流，任务是将一张马的照片变成斑马，反之亦然。请注意，其中有两个独立的生成器网络，以及两个不同的判别器。 图2.6 经过训练的CycleGAN可以欺骗两个判别器网络 如图所示，第一个生成器从属于不同分布（马）的图像开始学习生成符合目标分布（本例中是斑马）的图像，这样判别器就无法分辨从马的照片生成的图像是否真的是斑马的照片。与此同时--这也就是缩写中的Cycle前缀的含义--生成的假斑马通过不同的生成器被送到另一个方向（本例中是斑马到马），由另一边的另一个判别器来判断。创建这样一个循环，大大稳定了训练过程，这解决了GANs最初的一个问题。 有趣的是，此时，我们不需要匹配的马/斑马对作为基础事实（祝您好运，让他们匹配！）。从一组不相关的马匹图像和斑马照片开始，让生成器学习其任务，这已经超越了纯粹的监督环境。这个模型的意义远不止于此：生成器学会了如何在没有监督指导什么是什么的情况下，选择性地改变场景中物体的外观。没有任何信号指明鬃毛是鬃毛，腿是腿，但它们确实会被翻译成与其他动物的解剖学一致的东西。 2.2.3 把马变成斑马的网络 我们现在就可以使用此模型。CycleGAN网络已经在从ImageNet数据集中提取的（不相关的）马匹图像和斑马图像的数据集上进行了训练。该网络学习了把单张或多张马的图像全部变成斑马，而图像的其余部分则尽可能保持不变。虽然在过去的几千年里，人类并没有拼命去寻找一个能把马变成斑马的工具，但这项任务展示了这些架构在远程监督(distant supervision)下模拟复杂的现实世界过程的能力。虽然它们有其局限性，但有迹象表明，在不久的将来，我们将无法在视频直播中区分真假，这是一个棘手的问题，我们暂时可以不去理会。 使用预先训练好的CycleGAN将使我们有机会更近一步地观察一个网络--在本例中是一个生成器--是如何实现的。我们将使用我们的老朋友ResNet。我们将在屏幕外定义一个ResNetGenerator类。代码位于3_cyclegan.ipynb 文件的第一个单元格中，但目前与它的实现无关，而且在我们获得更多的PyTorch经验之前，它太复杂了，难以理解。现在，我们关注的是它能做什么，而不是它如何做到的。让我们用默认参数实例化这个类（code/p1ch2/3_cyclegan.ipynb），如： # In[2]: netG = ResNetGenerator() netG模型已经被创建，但它包含随机权重。我们前面提到，将运行一个已经在 horse2zebra 数据集上预先训练过的生成器模型，其训练集包含两组：分别为 1068 张和 1335 张马和斑马的图像。该数据集位于 http://mng.bz/8pKP 。模型的权重已经保存在一个.pth文件中，它只不过是模型的张量参数的pickle文件。我们可以使用模型的 load _state_dict 方法将这些加载到 ResNetGenerator 中： # In[3]: model_path = '../data/p1ch2/horse2zebra_0.4.0.pth' model_data = torch.load(model_path) netG.load_state_dict(model_data) 此时，netG已经获得了它在训练中获得的所有数据。请注意，这完全等同于我们在2.1.3节中从torchvision加载resnet101，只是torchvision.resnet101函数对我们隐藏了加载过程。 让我们将网络置于eval模式，就像对resnet101所做的那样： # In[4]: netG.eval() # Out[4]: ResNetGenerator( (model): Sequential( ... ) ) 按照我们之前的做法打印出模型，我们可以理解、考虑到它的作用，其实它是相当精简的。它获取一张图片，通过观察像素来识别其中的一匹或多匹马，并单独修改这些像素的值，使输出的图像看起来像一匹可信的斑马。我们不会在打印出来的图像中（或者说在源代码中）识别出任何类似斑马的东西：那是因为里面没有不存在任何类似斑马的东西。网络是只是一个框架--关键在于权重里。 我们准备加载一匹马的随机图像，看看我们的生成器会产生什么。首先，我们需要导入PIL和torchvision，如： # In[5]: from PIL import Image from torchvision import transforms 然后，我们定义了一些输入转换，以确保数据以正确的形状和大小进入网络： # In[6]: preprocess = transforms.Compose([transforms.Resize(256), transforms.ToTensor()]) 让我们打开一张马的文件(如图2.7所示)： 图2.7 一个男人骑着一匹烈马 # In[7]: img = Image.open(\"../data/p1ch2/horse.jpg\") img 好了，马背上有位老兄。(从图片上看，他待不久。)总之，让我们通过预处理，把它变成一个形状正确的变量： # In[8]: img_t = preprocess(img) batch_t = torch.unsqueeze(img_t, 0) 我们现在不用考虑细节，重要的是我们要跟着尝试。此时，可以将batch_t发送到我们的模型： # In[9]: batch_out = netG(batch_t) batch_out 现在是生成器的输出，我们可以将其转换回图像: # In[10]: out_t = (batch_out.data.squeeze() + 1.0) / 2.0 out_img = transforms.ToPILImage()(out_t) # out_img.save('../data/p1ch2/zebra.jpg') out_img # Out[10]: 哦，伙计。谁会那样骑斑马呢？生成的图像(图2.8)并不完美，但考虑到让网络发现有人(某种程度上)骑在马背上似乎是有点不寻常。值得重申的是，学习过程并没有经过直接监督，即就是，人类描绘了数万匹马，或者手动PS了数千条斑马条纹。生成器已经学会了生成一个能让判别器误认为那是斑马的图像，而且这个图像没有任何蹊跷（显然判别器从来没有去过马术比赛） 图2.8 一个男人骑着一匹斑马 已经使用对抗式训练或其他方法开发了许多有趣的生成器。其中一些能够创造出不存在的可信的人脸；另一些能够将草图转化为虚构景观的逼真的图片。生成式模型还在探索如何产生真实的声音、可信的文本和令人愉快的音乐。这些模型很可能成为未来支持创作过程的工具的基础。 严肃地说，很难夸大这种工作的意义。像我们刚刚下载的工具只会变得更高质量和更普遍。尤其是换脸技术，已经得到了媒体的广泛关注。搜索 \"deep fakes\"，会发现大量的示例内容1（不过我们必须注意到，有大量不安全的工作内容被标注为此类，对于互联网上的所有内容，请谨慎点击）。 到目前为止，我们已经有机会使用一个能看到图像的模型和一个能生成新图像的模型。我们将以一个涉及到更多基本要素的模型来结束我们的旅程：自然语言。 2.3 一种描述场景的预训练网络 为了获得涉及自然语言的模型的第一手经验，我们将使用由Ruotian Luo慷慨提供的预训练的图像字幕模型。2这是由Andrej Karpathy实现的NeuralTalk2模型。当呈现自然图像时，这种模型会生成描述场景的英文标题，如图2.9所示。该模型在庞大的图像数据集上进行训练，并配上了成对的句子描述，例如，“虎斑猫倚在木桌上，一只爪子放在激光鼠标上，另一只爪子放在黑色笔记本电脑上。”3 图2.9 字幕模型的概念 此字幕模型有两个相连的部分。模型的前半部分是一个网络，它学习生成场景的 \"描述性 \"数字表示（虎斑猫、激光鼠标、爪子），然后将这些数字表示作为后半部分的输入。后半部分是一个循环神经网络( recurrent neural network ) ，通过把这些数值描述放在一起，生成一个连贯的句子。模型的两半部分一起对图像-字幕对进行训练。 模型的后半部分之所以被称为循环，是因为它在后续的前向传递中生成其输出（单个单词），其中每个前向传递的输入包括前一个前向传递的输出。这就产生了下一个词对之前生成的词的依赖关系，正如我们在处理句子或一般序列时所期望的那样。 2.3.1 NeuralTalk2 NeuralTalk2模型保存在 https://github.com/deep-learning-withpytorch/ImageCaptioning.pytorch ，我们可以在data文件夹中放入一系列图片并运行以下脚本： python eval.py --model ./data/FC/fc-model.pth--infos_path ./data/FC/fc-infos.pkl --image_folder ./data 让我们试试我们的 horse.jpg。它描述到，“一个男子在沙滩上骑马”，相当正确。 现在，为了好玩，让我们看看我们的CycleGAN是否也能骗过这个NeuralTalk2模型。让我们在data文件夹中添加zebra.jpg图片，然后重新运行模型:\"一群斑马站在田野里\"。 好吧，它把动物弄对了，但它在图像中看到了不止一只斑马。当然，这并不是这个网络所见过的斑马的姿态，也没有见过骑在斑马上的骑手（带有一些虚假的斑马图案）。此外，在训练数据集中，斑马很有可能是以群体的形式被描绘出来的，所以我们可能查到一点偏差。字幕生成网络也没有描述骑手的情况。同样，这可能是出于同样的原因：在训练数据集中，网络没有显示骑在斑马上的骑手。不管怎么说，这是一项令人印象深刻的壮举：我们生成了一个不可能存在的情况下的假图像，而字幕网络也足够灵活到能够正确地识别出主题。 我们想强调的是，这样的东西，在深度学习出现之前，是极难实现的，但现在只要有一个对马或斑马一无所知的通用架构，以及一个图像及其描述的语料库（本例中是MS COCO数据集），就可以用不到一千行的代码来实现。没有硬编码的标准或语法--包括句子在内的所有东西都是从数据中的模式中产生的。 在某种程度上，最后一个案例中的网络体系结构比我们之前看到的复杂，因为它包括两个网络。 一种是循环的，但它是由相同的构件构建的，而所有这些构建块都是由PyTorch提供的。 在撰写本文时，像这样的模型更多的是作为应用研究或新奇项目而存在，而不是有明确定义和具体用途的东西。结果虽然很有希望，但还没有好到可以使用的程度......随着时间的推移（和额外的训练数据），我们应该期望这类模型能够向有视觉障碍的人描述世界，从视频中转录场景，并执行其他类似的任务。 2.4 Torch Hub   从深度学习的早期开始，就已经发布了预训练模型，但是直到PyTorch 1.0为止，还没有办法确保用户有统一的界面来获取它们。正如我们在本章前面所看到的，TorchVision是一个干净接口的优秀例子，但是正如我们在CycleGAN和NeuralTalk2中所见，其他作者选择了不同的设计理念。   PyTorch 1.0引入了Torch Hub机制，作者可以通过它在GitHub上发布具有或不具有预训练权重的模型，并利用PyTorch可理解的接口来公开它。这使得从第三方加载预训练模型就像加载TorchVision模型一样容易。   作者通过Torch Hub机制发布模型时，所需要做的就是在GitHub仓库的根目录下放置一个名为 hubconf.py的文件。该文件具有非常简单的结构： dependencies = ['torch', 'math'] def some_entry_fn(*args, **kwargs): model = build_some_model(*args, **kwargs) return model def another_entry_fn(*args, **kwargs): model = build_another_model(*args, **kwargs) return model 其中， dependencies = ['torch', 'math']   为了寻求有趣的预训练模型，我们现在可以搜索包含hubconf.py的GitHub仓库，并且马上就知道可以使用torch.hub模块加载它们。让我们看看在实践中是如何做到这一点的。为此，我们将返回到TorchVision，因为它提供了一个如何与Torch Hub交互的清晰示例。   让我们访问 https://github.com/pytorch/vision ，注意到它包含一个hubconf.py文件。很好，检查完毕。首先要做的是在该文件中查看 repo 的入口点 (the entry points)--我们稍后需要指定它们。在TorchVision的例子下，有两个：resnet18和resnet50。我们已经知道它们的作用：它们分别返回18层和50层的ResNet模型。我们还留意到，这些入口点函数包含一个预训练的关键字参数。如本章前面所述，如果为True，则返回的模型将使用从ImageNet获取的权重进行初始化。   现在我们知道了repo，入口点，还有一个有趣的关键字参数。这就是我们需要使用 torch.hub 加载模型的全部内容，甚至无需克隆 repo。没错，PyTorch 将为我们处理这些工作： import torch from torch import hub resnet18_model = hub.load('pytorch/vision:master', 'resnet18', pretrained=True) 其中，hub.load('pytorch/vision:master',   这将下载 pytorch/vision 存储库的master分支的快照连同权重，到本地目录 (默认为我们主目录中的 .torch/hub)，然后运行 resnet18 入口点函数，返回实例化的模型。根据环境的不同，Python可能会提示缺少一个模块，比如PIL。Torch Hub 不会安装缺失的依赖关系，但它会向我们报告，以便我们采取行动。   此时，我们可以用适当的参数调用返回的模型，对其进行前向传递，就像我们之前做的那样。好处在于，现在每一个通过这种机制发布的模型，我们都可以使用同样的方式访问，这远远超出了我们的视野范围。   请注意，入口点应该返回模型；但严格地说，它们不是被强制这样执行的。例如，我们可以有一个用于转换输入的入口点和另一个用于将输出概率转换为文本标签的入口点。或者，我们可以有一个入口点只用于模型，而另一个入口点则包括模型以及预处理和后处理步骤。通过开放这些选项，PyTorch开发人员为社区提供了足够的标准化和很大程度的灵活性。我们将拭目以待，看看从这种机遇中会诞生什么模式。   在撰写本文时，Torch Hub还很新，以这种方式发布的模型并不多。我们可以通过谷歌搜索“github.com hubconf.py”找到它们。希望未来随着更多作者通过这个渠道分享他们的模型，这个列表会越来越长。 2.5 结论   我们希望这是一个有趣的章节。我们花了一些时间尝试使用PyTorch创建的模型，这些模型被优化为执行特定任务。实际上，我们之中更具进取精神的人已经可以将其中一种模型放在网络服务器后面，并开展业务，与原始作者分享收益！4一旦我们了解了这些模型是如何构建的，我们也可以利用在这里学到的知识下载一个预先训练好的模型，并在稍有不同的任务上快速对其进行微调。   我们还将了解如何使用相同的构建块来构建在不同类型数据上处理不同问题的模型。PyTorch做得特别正确的一点是，以基本工具集的形式提供这些构建模块--从API的角度来看，PyTorch并不是一个非常庞大的库，尤其是与其他深度学习框架相比时。   本书并不侧重于完整的PyTorch API或回顾深度学习架构。相反，我们将建立有关这些构建块的实践知识。这样，您将能够在坚实的基础上使用优秀的在线文档和存储库。   从下一章开始，我们将开始一段旅程，使我们能够使用PyTorch从头开始教授像本章所述的计算机技能。我们还将学习，从预训练的网络开始并在新数据上进行微调，而不是从头开始，是解决现有数据点特别少的问题的有效方法。这也是预训练网络成为深度学习从业者应该拥有的重要工具的另一个原因。 是时候学习第一个基本构件了：tensors(张量)。 2.6 练习 将金毛猎犬的图像输入到马-斑马的模型中。 您需要对图像进行什么准备工作？ 输出结果是什么样的？ 在GitHub上搜索提供hubconf.py文件的项目。 返回了多少个仓库？ 找到一个带有hubconf.py的有趣项目。您可以从文档中了解该项目的目的吗？ 将项目加入到书签，并在完成本书后再来回看。你能理解该项目的实现吗？ 2.7 小结 预训练网络是一种已经在数据集上训练过的模型。这样的网络通常可以在加载网络参数后立即产生有用的结果。 通过了解如何使用预训练模型，我们可以将神经网络集成到项目中，而无需设计或训练它。 AlexNet和ResNet是两个深度卷积网络，它们在发布后的几年里为图像识别树立了新的基准。 生成式对抗网络（GANs）有两个部分--生成器和判别器--共同工作以产生与真实物品无法区分的输出。 CycleGAN使用的架构支持在两个不同类别的图像之间来回转换。 NeuralTalk2使用混合模型架构来消费图像并产生图像的文本描述。 Torch Hub是使用适当的hubconf.py文件从任何项目加载模型和权重的标准化方法。 1 Vox的文章《乔丹·皮尔模拟奥巴马的公益广告是对假新闻的双向警告》中介绍了一个相关的例子。作者：Aja Romano； http://mng.bz/dxBz(warning:coarse laguage) 。 2 我们拷贝了一份此代码， https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch. 3 Andrej Karpathy and Li Fei-Fei, “Deep Visual-Semantic Alignments for Generating Image Descriptions,” https://cs.stanford.edu/people/karpathy/cvpr2015.pdf. 4 联系发布者，获取特许使用的机会! By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "},"Chapter3/3.1.html":{"url":"Chapter3/3.1.html","title":"3. 从张量开始","keywords":"","body":"3. 从张量开始 本章包括：  - 理解张量--PyTorch中的基本数据结构  - 张量的索引和运算  - 与NumPy多维数组的交互操作  - 将计算转移到GPU上以提高速度   在上一章中，我们介绍了深度学习所实现的众多应用中的一部分。它们无一例外地包括获取某种形式的数据(如图像或文本)，并产生另一种形式的数据(如标签、数字或更多图像、文本)。从这个角度来看，深度学习其实就是构建一个能够将数据从一种表示形式转化为另一种表示形式的系统。这种转换是通过从一系列展示所需映射的例子中提取共性来驱动的。例如，系统可能会注意到狗的一般形状和金毛犬的典型颜色。通过结合这两种图像属性，系统可以将具有给定形状和颜色的图像正确地映射到金毛犬标签上，而不是黑色的实验室(或就此而言，黄褐色的山猫猫)。由此产生的系统可以消耗大量的类似输入，并为这些输入产生有意义的输出。   该过程首先将我们的输入转换为浮点数。我们将在第4章中介绍如何将图像像素转换为数字，如图3.1中的第一步所示(以及许多其他类型的数据)。但在这之前，在本章中，我们将学习如何在 PyTorch 中使用 tensors (张量) 来处理所有的浮点数。 3.1 浮点数世界 由于浮点数是网络处理信息的方式，我们需要一种方法将我们想要处理的现实世界的数据编码成网络可以消化的东西，然后将输出的数据解码成我们可以理解和使用的内容。 图3.1 深度神经网络学习如何将输入表示转化为输出表示。(注：神经元和输出的数量不成比例。) 深度神经网络通常会分阶段学习从一种形式的数据到另一种形式的数据的转换，这意味着每个阶段之间的部分转换数据可以被认为是一系列中间表示。对于图像识别来说，早期的表示可以是边缘检测或某些纹理（如毛皮）等。更深层次的表示可以捕获更复杂的结构，如耳朵、鼻子或眼睛。 一般来说，这种中间表示是浮点数的集合，它对输入进行表征，并以一种有助于描述输入如何映射到神经网络的输出的方式来捕获数据的结构。这种表征是针对当前任务的，可以从相关示例中学习的。这些浮点数的集合及其操作是现代人工智能的核心--我们将在本书中看到几个这样的示例。 请务必牢记，这些中间表示（如图3.1第二步所示）是将输入与前一层神经元的权重相结合的结果。每个中间表示对于之前的输入都是唯一的。 在开始将数据转换为浮点输入之前，我们首先必须对PyTorch如何处理和存储数据--作为输入、中间表示和输出--有一些扎实的了解。本章将专门讨论这个问题。 为此，PyTorch引入了一个基本的数据结构：tensor (张量)。我们在第2章对预训练网络进行推理时，已经碰到了张量。对于那些来自数学、物理学或工程学的人来说，张量这个术语与空间、参照系以及它们之间的变换的概念捆绑在一起。无论好坏，这些概念在这里均不适用。在深度学习的上下文中，张量是指向量和矩阵到任意维数的泛化，如图3.2所示。这一概念的另一个名称是multidimensional array (多维数组)。张量的维数与用于引用张量内标量值的索引数一致。 图3.2 张量是在PyTorch中表示数据的基础 PyTorch不是唯一处理多维数组的库。NumPy是迄今为止最受欢迎的多维数组库，可以说它现在已经成为数据科学的通用语言。PyTorch的特点是与NumPy的无缝互操作性，这为它带来了与Python中其他科学库的一流集成，比如SciPy (https://www.scipy.org) 、Scikit-Learning (https://scikit-learn.org) 和Pandas (https://pandas.pydata.org) 。 与NumPy数组相比，PyTorch张量具有一些超强的功能，例如能够在图形处理单元(GPU)上执行非常快速的操作、在多个设备或机器上分布式操作，以及跟踪创建它们的计算图的能力。这些都是实现现代深度学习库的重要功能。 在本章中，我们将介绍PyTorch张量，其中涵盖了它的相关基础知识，以便为本书其余部分的工作做准备。首先，我们将学习如何使用PyTorch张量库操纵张量。这包括诸如如何将数据存储在内存中，如何在固定时间内对任意大的张量执行某些操作以及前面提到的NumPy互操作性和GPU加速。如果要使张量成为我们编程工具箱中的首选工具，了解张量的功能和API是非常重要的。在下一章中，我们将充分利用这些知识，并学习如何以能够通过神经网络进行学习的方式表示几种不同类型的数据。 3.2 张量：多维数组 我们已经了解到，张量是PyTorch中的基本数据结构。 张量是一个数组：即一种存储数字集合的数据结构，这些数字可以使用索引单独访问，并且可以使用多个索引进行索引。 3.2.1 从Python列表到PyTorch张量 让我们看看列表索引的实际应用，这样我们就可以将其与张量索引进行比较。在Python中取一个包含三个数字的列表(.code/p1ch3/1_tensors.ipynb)： # In[1]: a = [1.0, 2.0, 1.0] 我们可以使用对应的从零开始的索引来访问列表的第一个元素： # In[2]: a[0] # Out[2]: 1.0 # In[3]: a[2] = 3.0 a # Out[3]: [1.0, 2.0, 3.0] 对于处理数字向量的简单 Python 程序来说，使用 Python 列表来存储向量是很常见的，比如二维线的坐标。正如我们将在下一章中所看到的，使用更有效的张量数据结构，可以表示许多类型的数据--从图像到时间序列，甚至是句子。通过定义张量上的操作(我们将在本章中探讨其中一些操作)，即使是使用高级（但不是特别快速）的语言（例如Python），我们也可以同时高效地切片和操作数据。 3.2.2 构建我们的第一个张量 让我们构造第一个PyTorch张量，并看看它的构造。它不必是一个特别有意义的张量，只需设置为包括三个1的列即可： # In[4]: import torch a = torch.ones(3) a # Out[4]: tensor([1., 1., 1.]) # In[5]: a[1] # Out[5]: tensor(1.) # In[6]: float(a[1]) # Out[6]: 1.0 # In[7]: a[2] = 2.0 a # Out[7]: tensor([1., 1., 2.]) 其中， # In[4]: import torch 导入torch模块后，我们调用一个函数来创建一个大小为3的（一维）张量，填充值为1.0。我们可以使用它从零开始的索引来访问一个元素，或者为其赋一个新值。虽然从表面上看，这个例子与数字对象的列表没有太大区别，但在本质上是完全不同的。 3.2.3 张量的本质 Python列表或数字元组是在内存中单独分配的Python对象的集合，如图 3.3 左侧所示。另一方面，PyTorch tensors或NumPy数组从视图上看，(通常)是分布在连续内存块上，包含有unboxed C数字类型(译者注：即不涉及指针或堆分配的类型)，而不是Python对象。在这种情况下，如图3.3右侧所示，每个元素都是一个32位(4 字节)的浮点数。这意味着存储1,000,000个浮点数的1D(一维)张量恰好需要4,000,000个连续的字节，外加元数据(如尺寸和数值类型)的少量开销。 图3.3 Python对象(装箱)数值与张量(未装箱数组)数值的对比 假设我们有一个要用来表示几何对象的坐标列表：可能是一个顶点位于坐标(4，1)、(5，3)和(2，1)的2D三角形。这个例子与深度学习不是特别相关，但是很容易理解。我们可以使用一维张量，将X存储在偶数索引中，将Y存储在奇数索引中，而不是像我们前面所做的那样，在Python列表中将坐标作为数字，如下所示： # In[8]: points = torch.zeros(6) points[0] = 4.0 points[1] = 1.0 points[2] = 5.0 points[3] = 3.0 points[4] = 2.0 points[5] = 1.0 其中， # In[8]: points = torch.zeros(6) 我们还可以将Python列表传递给构造函数，效果相同： # In[9]: points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0]) points # Out[9]: tensor([4., 1., 5., 3., 2., 1.]) 为了得到第一个点的坐标，我们做如下操作： # In[10]: float(points[0]), float(points[1]) # Out[10]: (4.0, 1.0) 这是可以的，尽管让第一个索引引用单个二维点而不是点坐标是可行的。为此，我们可以使用2D张量： # In[11]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) points # Out[11]: tensor([[4., 1.], [5., 3.], [2., 1.]]) 在这里，我们向构造函数传递一串列表。我们可以查询张量的形状： # In[12]: points.shape # Out[12]: torch.Size([3, 2]) 这让我们知道了张量在每个维度上的尺寸。我们也可以使用zeros或ones来初始化张量，以元组形式提供尺寸： # In[13]: points = torch.zeros(3, 2) points # Out[13]: tensor([[0., 0.], [0., 0.], [0., 0.]]) 现在我们可以使用两个索引访问张量中的单个元素： # In[14]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) points # Out[14]: tensor([[4., 1.], [5., 3.], [2., 1.]]) # In[15]: points[0, 1] # Out[15]: tensor(1.) 这将返回我们数据集中第0个点的Y坐标。我们还可以像以前一样访问张量中的第一个元素，以获取第一个点的2D坐标： # In[16]: points[0] # Out[16]: tensor([4., 1.]) 输出的是另一个张量，它展示了同一基础数据的不同视图。新的张量是一个尺寸为2的1D张量，引用了points张量中第一行的值。这是否意味着分配了一个新的内存块，将值复制到其中，然后新的内存被包裹在一个新的张量对象中返回？不，因为这样的效率很低，尤其是当我们有数百万个点的时候。我们将在本章稍后的3.7节介绍张量的视图时，重新讨论张量的存储方式。 3.3 索引张量 如果我们需要获取一个除第一个点以外的所有点的张量怎么办呢？使用范围索引表示法很容易做到这一点，该表示法也适用于标准Python列表。提醒一下： # In[53]: some_list = list(range(6)) some_list[:] some_list[1:4] some_list[1:] some_list[:4] some_list[:-1] some_list[1:4:2] 其中， # In[53]: some_list = list(range(6)) some_list[:] 为了实现我们的目标，我们可以对PyTorch张量使用相同的表示法，还有额外的好处是，就像在 NumPy和其他Python科学库中一样，我们可以对张量的每个维度使用范围索引： # In[54]: points[1:] points[1:, :] points[1:, 0] points[None] 其中， # In[54]: points[1:] 除了使用范围之外，PyTorch 还提供了一种强大的索引形式，称为高级索引(advanced indexing)，我们将在下一章中进一步探索。 3.4 命名张量 张量的维度（或轴）通常会对像素位置或颜色通道等进行索引。这意味着当我们要对一个张量进行索引时，我们需要记住维度的顺序，并据此相应地编写我们的索引。由于数据是通过多个张量进行转换的，因此跟踪哪个维度包含什么数据可能会容易出错。 具体来说，假设我们有一个3D张量，如2.1.4节中的img_t(为简单起见，我们这里将使用虚拟数据)，我们希望将其转换为灰度。我们查找颜色的典型权重以得出单个亮度值：1 # In[2]: img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns] weights = torch.tensor([0.2126, 0.7152, 0.0722]) 我们还通常希望代码能够泛化-例如，从表示为具有高度和宽度维度的2D张量的灰度图像到添加第三个通道维度的彩色图像（如RGB），或者从单张图像到一批图像。在2.1.4节中，我们在batch_t中引入了另一个批处理维度；在这里，我们假设有一个批处理维度设置为2： # In[3]: batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns] 所以有时RGB通道在0维，有时在1维。但我们可以通过从末尾数起的方式来概括：它们总是在维度-3，即从末尾数起的第三个维度。因此，偷懒版的、未加权的平均数(mean)可以写成如下： # In[4]: img_gray_naive = img_t.mean(-3) batch_gray_naive = batch_t.mean(-3) img_gray_naive.shape, batch_gray_naive.shape # Out[4]: (torch.Size([5, 5]), torch.Size([2, 5, 5])) 但现在我们加入了权重。PyTorch允许我们将形状(shape)相同的量相乘，或者将形状里一个操作数(one operand)在指定维度中为1的量相乘。它还会自动追加尺寸为1的leading dimensions(LDA)。形状为(2, 3, 5, 5)的 batch_t 乘以形状(3, 1, 1)的 unsqueezed_weights，得到形状(2, 3, 5, 5)的张量，然后我们可以从末端(三个通道)对第三个维度进行求和： # In[5]: unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1) img_weights = (img_t * unsqueezed_weights) batch_weights = (batch_t * unsqueezed_weights) img_gray_weighted = img_weights.sum(-3) batch_gray_weighted = batch_weights.sum(-3) batch_weights.shape, batch_t.shape, unsqueezed_weights.shape # Out[5]: (torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1])) 因为这样做变得杂乱无章--为了提高效率，PyTorch函数 einsum (改编自 NumPy) 指定了一种索引迷你语言(indexing mini-language)2，为这些乘积的和提供了维度的索引名称。就像在Python中一样，广播 -- 一种总结未命名事物的形式 -- 是用三个点 '...'来完成的；但是不用太担心einsum，因为我们在下面内容中不会使用它： # In[6]: img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights) batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights) batch_gray_weighted_fancy.shape # Out[6]: torch.Size([2, 5, 5]) 正如我们所看到的，这涉及到相当多的记账工作(bookkeeping)。这很容易出错，特别是当我们的代码中创建和使用张量的位置相隔很远的时候。这已经引起了从业者的注意，因此有人建议给这个维度起个名字3。 PyTorch 1.3 增加了命名张量作为实验性功能(请参见 https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html 和 https://pytorch.org/docs/stable/named_tensor.html) 。 张量的factory函数，如tensor和rand，需要一个names参数。names应该是一个字符串序列： # In[7]: weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels']) weights_named # Out[7]: tensor([0.2126, 0.7152, 0.0722], names=('channels',)) 当我们已经有了一个张量，并且想要添加名称（但不改变现有的名称）时，我们可以对它调用refine_names方法。与索引类似，省略号(...)允许你省略任意数量的维度。通过rename sibling方法，你也可以覆盖或删除（通过传递None）现有的名字。 当我们已经有了张量并想要添加名称（但不更改现有名称）时，可以在其上调用方法fine_names。 与索引类似，省略号（...）允许您省略任意数量的维度。通过rename同级(sibling)方法，您还可以覆盖或删除（通过传入None）现有名称： # In[8]: img_named = img_t.refine_names(..., 'channels', 'rows', 'columns') batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns') print(\"img named:\", img_named.shape, img_named.names) print(\"batch named:\", batch_named.shape, batch_named.names) # Out[8]: img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns') batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns') 对于有两个输入的操作，除了常规的维度检查（即尺寸是否相同，或其中一个尺寸是否为1并可广播给另一个）之外，PyTorch现在还将为我们检查名称。到目前为止，它不会自动对齐维度，因此我们需要明确地进行此操作。align_as方法会返回一个添加了缺失维度的张量，并将现有的维度按正确的顺序排列： # In[9]: weights_aligned = weights_named.align_as(img_named) weights_aligned.shape, weights_aligned.names # Out[9]: (torch.Size([3, 1, 1]), ('channels', 'rows', 'columns')) 接受维度参数（例如sum）的函数也接受命名维度： # In[10]: gray_named = (img_named * weights_aligned).sum('channels') gray_named.shape, gray_named.names # Out[10]: (torch.Size([5, 5]), ('rows', 'columns')) 如果尝试将维度与不同的名称组合在一起，则会出现错误： gray_named = (img_named[..., :3] * weights_named).sum('channels') attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match. 如果要在对命名张量进行运算的函数之外使用该张量，则需要通过将名称重命名为None来删除名称。以下内容使我们回到了未命名维度的世界： # In[12]: gray_plain = gray_named.rename(None) gray_plain.shape, gray_plain.names # Out[12]: (torch.Size([5, 5]), (None, None)) 在撰写本文时，考虑到该特性的实验性质，为了避免在索引和对齐方面浪费时间，我们将坚持在本书的其余部分使用未命名的张量。如果PyTorch论坛有任何指示的话，命名张量有可能消除许多对齐错误的来源(是个相当令人头疼的来源)。看看它们会被多大程度地采用会是一件有趣的事情。 3.5 张量元素类型 到目前为止，我们已经介绍了张量工作的基本原理，但是尚未涉及可以在张量中存储哪些数值类型。正如我们在3.2节中所提示的那样，使用标准Python数字类型可能不是最优的，原因有以下几种： Python中的数字(Numbers) 是 对象(objects)。例如，浮点数在计算机上可能只需要比32位来表示，而Python会将其转换为具有引用计数的完整Python对象。这种操作被称为装箱(boxing)。如果我们需要存储少量的数字，不成问题，但是分配数百万的量则效率很低。 Python中的列表(Lists) 是为了对象的顺序集合(sequential collections)。没有定义两个向量的点积或向量求和的高效操作。而且，Python列表没有办法优化其内容在内存中的布局，因为它们是指向Python对象 (任何种类，而不仅仅是数字) 的指针的可索引集合。最后，Python列表是一维的，虽然我们可以创建列表的列表，但这同样也是非常低效的。 与优化、编译后的代码相比，Python解释器的速度很慢。对大量的数值数据集合进行数学运算，使用像C这样的可编译的低级语言编写的优化代码工作速度可以快得多。 由于这些原因，数据科学库依赖于NumPy或引入专用数据结构（例如PyTorch张量）。它提供了数值的数据结构和对其进行相关操作的高效低层次的实现，并包装在一个方便的高级API中。要实现这一点，张量内的对象必须全部是相同类型的数字，并且PyTorch必须跟踪此数字类型。 3.5.1 使用dtype指定数值类型 张量构造函数（也就是像tensor、zeros和ones这样的函数）的dtype参数指定了张量中包含的数值数据（d）类型。数据类型指定了张量可以容纳的值（整数与浮点数）和每个值的字节数。4 dtype参数有意类似于同名的标准NumPy参数。下面是dtype参数的可能值列表: torch.float32或torch.float：32位浮点 torch.float64或torch.double：64位，双精度浮点 torch.float16或torch.half：16位半精度浮点 torch.int8：带符号的8位整数 torch.uint8：无符号的8位整数 torch.int16或torch.short：带符号的16位整数 torch.int32或torch.int：带符号的32位整数 torch.int64或torch.long：带符号的64位整数 torch.bool：布尔值 张量的默认数据类型是32位浮点数。 3.5.2 适合各种场合的dtype 正如我们将在以后的章节中看到的那样，神经网络中发生的计算通常以32位浮点精度执行。 更高的精度（例如64位）并不能改善模型的准确性，并且需要更多的内存和计算时间。16位浮点半精度数据类型在标准CPU中并不存在，而在现代GPU上提供了这种类型。如果需要的话，可以切换到半精度以减少神经网络模型的占用空间，而对精度的影响很小。 张量可以用作其他张量的索引。在这种情况下，PyTorch期望索引张量具有64位整数数据类型。 创建一个以整数为参数的张量，例如使用torch.tensor（[2，2]），默认情况下将创建一个64位整数张量。 因此，我们将大部分时间用于处理float32和int64。 张量可以用作其他张量的索引。在本例中，PyTorch期望索引张量具有64位整数数据类型。默认情况下，使用整数作为参数创建张量(如使用torch.tensor([2，2])将创建一个64位整数张量。因此，我们将把大部分时间用于处理float32和int64上。 最后，关于张量的断言（例如，points > 1.0）会产生表示每个单独元素是否满足条件的bool张量.简而言之，这些都是数字类型。 3.5.3 管理张量的dtype属性 为了分配正确数值类型的张量，我们可以将正确的数据类型指定为构造函数的参数。例如： # In[47]: double_points = torch.ones(10, 2, dtype=torch.double) short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short) 我们可以通过访问相应的属性来了解张量的dtype： # In[48]: short_points.dtype # Out[48]: torch.int16 我们还可以使用相应的强制转换方法将张量构造函数的输出强制转换为正确的类型，例如 # In[49]: double_points = torch.zeros(10, 2).double() short_points = torch.ones(10, 2).short() 或更方便的方法to： # In[50]: double_points = torch.zeros(10, 2).to(torch.double) short_points = torch.ones(10, 2).to(dtype=torch.short) 在后台，to检查是否需要转换，如果需要，就进行转换。像float这样的dtype命名的强制转换方法是to的简写，但是to方法可以接受其他参数，我们将在3.9节中讨论这些。 在操作中混合输入不同类型时，输入将自动转换为较大的类型。因此，如果我们要进行32位计算，则需要确保所有输入（最多）是32位： # In[51]: points_64 = torch.rand(5, dtype=torch.double) points_short = points_64.to(torch.short) points_64 * points_short # works from PyTorch 1.3 onwards # Out[51]: tensor([0., 0., 0., 0., 0.], dtype=torch.float64) 其中， # In[51]: points_64 = torch.rand(5, dtype=torch.double) 3.6 张量API 至此，我们知道了什么是PyTorch张量，以及它们的工作原理。在我们结束之前，值得一看的是 PyTorch提供的张量操作。在这里列出所有的操作是没有什么用处的。相反，我们将对API有一个大致的了解，并就如何在 http://pytorch.org/docs 的在线文档中找到相关内容提供一些指导。 首先，绝大多数关于张量和张量之间的操作都可以在 torch 模块中找到，也可以作为张量对象的方法来调用。例如，我们前面遇到的transpose函数就可以在 torch 模块中调用： # In[71]: a = torch.ones(3, 2) a_t = torch.transpose(a, 0, 1) a.shape, a_t.shape # Out[71]: (torch.Size([3, 2]), torch.Size([2, 3])) 或者作为张量a的方法被调用： # In[72]: a = torch.ones(3, 2) a_t = a.transpose(0, 1) a.shape, a_t.shape # Out[72]: (torch.Size([3, 2]), torch.Size([2, 3])) 这两种形式没有区别，可以互换使用。 我们前面提到了在线文档（http://pytorch.org/docs)。它们非常详尽，而且条理清晰，张量操作被分成了几组： 创建操作(Creation ops) --构造张量的函数，比如 ones 和 from_numpy 索引、切片、连接、转换操作(indexing, slicing, joining, mutating ops) --改变张量的形状、步长或内容的函数，如transpose。 数学运算(Math ops)--通过计算来操作张量内容的函数。 逐点运算(Pointwise ops)--通过将函数独立地应用于每个元素来获得新张量的函数，如abs和cos 归约运算(Reduction ops)--通过遍历张量计算聚合值的函数，如mean, std, 和norm 比较运算(Comparison ops)--用于评估张量中数字关系断言的函数，如equal和max 频谱运算(Spectral ops)--变换到频域并在频域中运算的函数，如stft和hamming_window 其他运算(Other operations)--对向量(如cross)或矩阵(如trace)进行运算的特殊函数。 BLAS和LAPACK运算--遵循基本线性代数子程序（BLAS）规范的函数，用于标量，矢量-矢量，矩阵-矢量和矩阵-矩阵运算 随机抽样(Random sampling)--通过从概率分布中随机抽取来生成值的函数，如randn和normal 序列化(Serialization)--保存和加载张量的函数，如load和save 并行化(Parallelism)--用于控制并行CPU执行的线程数量的函数，如set_num_thread 花一些时间来使用常规的张量API。本章提供了启用这种交互式探索的所有先决条件。从下一章开始，我们还将在本书中遇到一些张量运算。 3.7 张量：对存储的图形化 现在是时候让我们更仔细地看看代码的实现了。张量中的值被分配在由torch.Storage实例管理的连续内存块中。一个storage一个由数字数据组成的一维数组：即包含给定类型数字的连续内存块，例如 float（32 位代表浮点数）或 int64（64 位代表整数）。PyTorch当中 Tensor 实例就是这样一个存储(torage)实例的体现，它能够使用偏移量和每个维度的步长(stride)来索引到该存储。5 图3.4 Tensors是Storage实例的视图 多个张量可以索引同一存储，即使它们对数据的索引并不相同。我们可以在图3.4中看到一个示例。实际上，当我们在3.2节中请求points[0]时，我们得到的是以points张量保存的同一存储空间——不是它的全部，并是具有不同的维度（1D与2D）。但是，基础内存仅分配一次，因此，无论Storage实例管理的数据大小如何，都可以快速完成对数据的备用张量视图的创建。 3.7.1 索引到存储 让我们看看在2D点中实际上如何建立存储索引。 使用.storage属性可以访问给定张量的存储： # In[17]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) points.storage() # Out[17]: 4.0 1.0 5.0 3.0 2.0 1.0 [torch.FloatStorage of size 6] 尽管张量报告自己有三行两列，但它下面的存储是一个大小为6的连续数组。在这个意义上，张量只知道如何将一对索引转换为存储中的某个位置。。 我们也可以手动索引到一个存储。例如： # In[18]: points_storage = points.storage() points_storage[0] # Out[18]: 4.0 # In[19]: points.storage()[1] # Out[19]: 1.0 我们不能用两个索引来索引一个二维张量的存储。一个存储的布局总是一维的，不管可能引用它的任何和所有张量的维度如何。 在这一点上，更改存储的值会导致更改其引用张量的内容不足为奇： # In[20]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) points_storage = points.storage() points_storage[0] = 2.0 points # Out[20]: tensor([[2., 1.], [5., 3.], [2., 1.]]) 3.7.2 修改存储的值：就地操作（In-place operations) 除了上一节介绍的对张量的操作外，还有少量的操作只作为Tensor对象的方法存在。它们可以通过名称中的下划线来识别，比如zero_，它表示该方法通过就地(in-place)修改输入而不是创建一个新的输出张量并返回它。例如，zero_方法将输入的所有元素置为零。任何没有尾部下划线的方法都不会改变源张量，而是返回一个新的张量： # In[73]: a = torch.ones(3, 2) # In[74]: a.zero_() a # Out[74]: tensor([[0., 0.], [0., 0.], [0., 0.]]) 3.8 张量元数据：大小(size)、偏移(offset)和步长(stride) 为了索引存储(storage)，张量依赖于几条信息以及它们的存储，明确地定义它们：大小(size)、偏移(offset)和步长(stride)。这些信息相互作用的方式如图3.5所示。大小（或形状shape，以NumPy的说法）是一个元组(tuple)，指示张量表示的每个维度上有多少个元素。存储的偏移是与张量中的第一个元素相对应的存储中的索引。步长是存储中需要跳过的元素数量，以便沿每个维度获取下一个元素。 图3.5 张量的偏移，大小和步长之间的关系。张量在这里是较大存储的视图，就像在创建较大张量时可能已分配的存储那样 3.8.1 另一个张量的存储视图 我们可以通过提供相应的索引来获得张量中的第二个点： # In[21]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) second_point = points[1] second_point.storage_offset() # Out[21]: 2 # In[22]: second_point.size() # Out[22]: torch.Size([2]) 生成的张量在存储中的偏移为2（因为我们需要跳过第一个点，它有两项），并且由于张量是一维的，size是一个包含一个元素的Size类的实例。需要注意的是，这与张量对象shape属性中包含的信息相同: # In[23]: second_point.shape # Out[23]: torch.Size([2]) stride是一个元组，表示当索引在每个维度上增加1时必须跳过的存储元素的数量。例如，我们的points张量的步长为(2，1): # In[24]: points.stride() # Out[24]: (2, 1) 在2D张量中访问元素i，j的结果是访问存储器中的storage_offset + stride [0] * i + stride [1] * j元素。偏移通常为零；如果此张量是为容纳更大张量而创建的存储的视图，则偏移量可能为正值。 Tensor和Storage之间的这种间接性使得一些操作的开销变得很低，比如转置张量或提取子张量，因为它们不会导致内存重新分配。取而代之的是，它们会分配一个新的包括不同大小、存储偏移量或步长的Tensor对象。 当我们索引特定点并看到存储偏移量增加时，我们已经提取了一个子张量。让我们来看看大小和步长是如何变化的： # In[25]: second_point = points[1] second_point.size() # Out[25]: torch.Size([2]) # In[26]: second_point.storage_offset() # Out[26]: 2 # In[27]: second_point.stride() # Out[27]: (1,) 下面的代码则是，如我们所期望的那样，子张量具有较小的维度，同时仍索引与原始的points张量相同的存储。这也意味着更改子张量将对原始张量产生副作用： # In[28]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) second_point = points[1] second_point[0] = 10.0 points # Out[28]: tensor([[ 4., 1.], [10., 3.], [ 2., 1.]]) 这种操作可能并不是我们想要的，因此可以选择拷贝子张量的值到一份新的张量里面去： # In[29]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) second_point = points[1].clone() second_point[0] = 10.0 points # Out[29]: tensor([[4., 1.], [5., 3.], [2., 1.]]) 3.8.2 无需复制的转置 现在我们来试试转置。让我们采用points张量，它在行中有单独的点，在列中有对应的X和Y坐标，将他转置，使单个点换到列中。我们借此机会介绍一下t函数，它是二维张量转置的速记替代方法： # In[30]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) points # Out[30]: tensor([[4., 1.], [5., 3.], [2., 1.]]) # In[31]: points_t = points.t() points_t # Out[31]: tensor([[4., 5., 2.], [1., 3., 1.]]) 贴士：为了帮助您更好地理解张量的原理，当我们在本节中学习代码时，不妨拿起一支铅笔和一张纸，在图3.5中这样的图表上涂鸦。 我们可以轻松地验证两个张量共享相同的存储： # In[32]: id(points.storage()) == id(points_t.storage()) # Out[32]: True 而它们的区别只在于形状和步长： # In[33]: points.stride() # Out[33]: (2, 1) # In[34]: points_t.stride() # Out[34]: (1, 2) 这告诉我们，以点为单位增加第一个索引--例如，从点[0,0]到点[1,0]--将沿存储跳过两个元素，而增加第二个索引--从点[0,0]到点[0,1]--将沿存储跳过一个。换句话说，存储空间按顺序逐行存放张量中的元素。 这告诉我们，在points中的第一个索引增加1，例如，从points[0,0]到points[1,0]--将沿着存储跳过两个元素，而增加第二个索引：从points[0,0]到points[0,1]--将沿存储跳过1一个元素。换句话说，存储空间按顺序逐行存放张量中的元素。 我们可以将points转置为points_t，如图3.6所示。我们先改变步长中元素的顺序。之后，增加行（张量的第一个索引）将沿存储空间跳过一个元素，就像我们沿points的列移动时一样。这就是转置的定义。没有分配新的内存：仅通过创建步长顺序与原始顺序不同的新Tensor实例来获得转置。 图3.6 对张量进行转置操作 3.8.3 在更高的维度上进行转置 PyTorch中的转置不仅限于矩阵。我们可以通过指定两个维度来进行多维数组的转置（翻转形状和步长）： # In[35]: some_t = torch.ones(3, 4, 5) transpose_t = some_t.transpose(0, 2) some_t.shape # Out[35]: torch.Size([3, 4, 5]) # In[36]: transpose_t.shape # Out[36]: torch.Size([5, 4, 3]) # In[37]: some_t.stride() # Out[37]: (20, 5, 1) # In[38]: transpose_t.stride() # Out[38]: (1, 5, 20) 一个张量的值在存储中从最右边的维度开始排列（也就是说，对于一个二维张量来说，沿着行移动）被定义为连续(contiguous)的。连续张量很方便，因为我们可以按顺序高效访问它们，而不需要在存储中跳来跳去（由于现代CPU上的内存访问方式，提高数据的位置性可以提高性能）。当然，这个优势取决于算法的访问方式： 3.8.4 连续张量（contiguous tensors） PyTorch 中的一些张量操作只适用于连续的张量，例如我们将在下一章中遇到的view。在这种情况下，PyTorch会抛出一个信息异常，并要求我们显式调用contiguous。值得注意的是，如果张量已经是连续的，那么调用contiguous不会有任何作用（也不会影响性能）。 在我们的例子中，points是连续的，而它的转置不是： # In[39]: points.is_contiguous() # Out[39]: True # In[40]: points_t.is_contiguous() # Out[40]: False 我们可以用contiguous方法从一个非连续的张量中得到一个新的连续张量。张量的内容不变，但步长会改变，存储也会改变： # In[41]: points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) points_t = points.t() points_t # Out[41]: tensor([[4., 5., 2.], [1., 3., 1.]]) # In[42]: points_t.storage() # Out[42]: 4.0 1.0 5.0 3.0 2.0 1.0 [torch.FloatStorage of size 6] # In[43]: points_t.stride() # Out[43]: (1, 2) # In[44]: points_t_cont = points_t.contiguous() points_t_cont # Out[44]: tensor([[4., 5., 2.], [1., 3., 1.]]) # In[45]: points_t_cont.stride() # Out[45]: (3, 1) # In[46]: points_t_cont.storage() # Out[46]: 4.0 5.0 2.0 1.0 3.0 1.0 [torch.FloatStorage of size 6] 请注意，现在已经对存储进行了reshuffled操作，以便在新存储中逐行布置元素。步长已更改以反映新的存储布局。 作为回顾，图3.7再次显示了我们的图。希望现在，我们已经对张量的构建方式有了很好的了解。 图3.7 张量的偏移，大小和步长之间的关系。张量在这里是较大存储的视图，就像在创建较大张量时可能已分配的存储那样 3.9 移动张量至GPU 到目前为止，在本章中，当我们谈论存储时，是指CPU上的内存。 PyTorch张量也可以存储在另一种处理器上：图形处理单元（GPU）。每个PyTorch张量都可以传输到GPU中，以便执行大规模的并行快速计算。对张量进行的所有操作都将使用PyTorch附带的GPU专用例程来执行。 3.10 NumPy互操作性 3.11 广义张量也是张量 3.12 序列化张量 3.13 结论 3.14 练习 3.15 小结 1 由于感知并非毫无意义，人们提出了许多衡量的标准。例如，请参见：https://en.wikipedia.org/wiki/Luma_(video) . 2 蒂姆·罗克塔舍尔(TimRocktäschel)的博客文章“Einsum is All You Need—Einstein Summation in Deep Learning” (https://rockt.github.io/2018/04/30/einsum) 提供了很好的概述。 3 See Sasha Rush, “Tensor Considered Harmful,” Harvardnlp, http://nlp.seas.harvard.edu/NamedTensor. 4 5 By paper2Fox，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2020-12-25 15:20:22 "}}