# <p align="right">2.预训练网络</p>
***

>本章包括：  
&emsp;&emsp;- 运行预训练的图像识别模型  
&emsp;&emsp;- GANs和CycleGAN的介绍  
&emsp;&emsp;- 可以产生图像文字描述的字幕模型  
&emsp;&emsp;- 通过Torch Hub共享模型  



## 2.1 <span id='chap2-1'>一种可识别图像主体的预训练网络</span>

## 2.2 <span id='chap2-2'>一种从伪到真的预训练模型</span>
## 2.3 <span id='chap2-3'>一种描述场景的预训练网络</span>
## 2.4 <span id='chap2-4'>Torch Hub</span>

&emsp;&emsp;从深度学习的早期开始，就已经发布了预训练模型，但是直到PyTorch 1.0为止，还没有办法确保用户有统一的界面来获取它们。正如我们在本章前面所看到的，TorchVision是一个干净接口的优秀例子，但是正如我们在CycleGAN和NeuralTalk2中所见，其他作者选择了不同的设计理念。

&emsp;&emsp;PyTorch 1.0引入了Torch Hub机制，作者可以通过它在GitHub上发布具有或不具有预训练权重的模型，并利用PyTorch可理解的接口来公开它。这使得从第三方加载预训练模型就像加载TorchVision模型一样容易。

&emsp;&emsp;作者通过Torch Hub机制发布模型时，所需要做的就是在GitHub仓库的根目录下放置一个名为 `hubconf.py`的文件。该文件具有非常简单的结构：

```python
dependencies = ['torch', 'math']
 
def some_entry_fn(*args, **kwargs):
    model = build_some_model(*args, **kwargs)
    return model
 
def another_entry_fn(*args, **kwargs):
    model = build_another_model(*args, **kwargs)
    return model
```
```python
其中，
dependencies = ['torch', 'math']    <-----------可选的代码依赖的模块列表
 
def some_entry_fn(*args, **kwargs): <-----------要作为存储库的入口点向用户开放的一个或多个函数。
                                    这些函数应根据参数初始化模型并返回。
    ...
    ...
```

&emsp;&emsp;为了寻求有趣的预训练模型，我们现在可以搜索包含`hubconf.py`的GitHub仓库，并且马上就知道可以使用`torch.hub`模块加载它们。让我们看看在实践中是如何做到这一点的。为此，我们将返回到TorchVision，因为它提供了一个如何与Torch Hub交互的清晰示例。

&emsp;&emsp;让我们访问 https://github.com/pytorch/vision ，注意到它包含一个`hubconf.py`文件。很好，检查完毕。首先要做的是在该文件中查看 repo 的入口点 (the entry points)--我们稍后需要指定它们。在TorchVision的例子下，有两个：`resnet18`和`resnet50`。我们已经知道它们的作用：它们分别返回18层和50层的ResNet模型。我们还留意到，这些入口点函数包含一个预训练的关键字参数。如本章前面所述，如果为True，则返回的模型将使用从ImageNet获取的权重进行初始化。

&emsp;&emsp;现在我们知道了repo，入口点，还有一个有趣的关键字参数。这就是我们需要使用 torch.hub 加载模型的全部内容，甚至无需克隆 repo。没错，PyTorch 将为我们处理这些工作：

```python
import torch
from torch import hub
 
resnet18_model = hub.load('pytorch/vision:master',
                           'resnet18',
                           pretrained=True)
```
```python
其中，hub.load('pytorch/vision:master',    <----------GitHub repo的name和branch
                'resnet18',                <----------入口点函数的name
                pretrained=True)           <----------关键字参数
```

&emsp;&emsp;这将下载 pytorch/vision 存储库的master分支的快照连同权重，到本地目录 (默认为我们主目录中的 .torch/hub)，然后运行 resnet18 入口点函数，返回实例化的模型。根据环境的不同，Python可能会提示缺少一个模块，比如`PIL`。Torch Hub 不会安装缺失的依赖关系，但它会向我们报告，以便我们采取行动。

&emsp;&emsp;此时，我们可以用适当的参数调用返回的模型，对其进行前向传递，就像我们之前做的那样。好处在于，现在每一个通过这种机制发布的模型，我们都可以使用同样的方式访问，这远远超出了我们的视野范围。

&emsp;&emsp;**请注意**，入口点应该返回模型；但严格地说，它们不是被强制这样执行的。例如，我们可以有一个用于转换输入的入口点和另一个用于将输出概率转换为文本标签的入口点。或者，我们可以有一个入口点只用于模型，而另一个入口点则包括模型以及预处理和后处理步骤。通过开放这些选项，PyTorch开发人员为社区提供了足够的标准化和很大程度的灵活性。我们将拭目以待，看看从这种机遇中会诞生什么模式。

&emsp;&emsp;在撰写本文时，Torch Hub还很新，以这种方式发布的模型并不多。我们可以通过谷歌搜索“github.com hubconf.py”找到它们。希望未来随着更多作者通过这个渠道分享他们的模型，这个列表会越来越长。

## 2.5 <span id='chap2-5'>结论</span>

&emsp;&emsp;我们希望这是一个有趣的章节。我们花了一些时间尝试使用PyTorch创建的模型，这些模型被优化为执行特定任务。实际上，我们之中更具进取精神的人已经可以将其中一种模型放在网络服务器后面，并开展业务，与原始作者分享收益！[<sup>4</sup>](#jump2-4)一旦我们了解了这些模型是如何构建的，我们也可以利用在这里学到的知识下载一个预先训练好的模型，并在稍有不同的任务上快速对其进行微调。

&emsp;&emsp;我们还将了解如何使用相同的构建块来构建在不同类型数据上处理不同问题的模型。PyTorch做得特别正确的一点是，以基本工具集的形式提供这些构建模块--从API的角度来看，PyTorch并不是一个非常庞大的库，尤其是与其他深度学习框架相比时。

&emsp;&emsp;本书并不侧重于完整的PyTorch API或回顾深度学习架构。相反，我们将建立有关这些构建块的实践知识。这样，您将能够在坚实的基础上使用优秀的在线文档和存储库。

&emsp;&emsp;从下一章开始，我们将开始一段旅程，使我们能够使用PyTorch从头开始教授像本章所述的计算机技能。我们还将学习，从预训练的网络开始并在新数据上进行微调，而不是从头开始，是解决现有数据点特别少的问题的有效方法。这也是预训练网络成为深度学习从业者应该拥有的重要工具的另一个原因。
是时候学习第一个基本构件了：**tensors(张量)**。

## 2.6 <span id='chap2-6'>练习</span>

1. 将金毛猎犬的图像输入到马-斑马的模型中。
    1. 您需要对图像进行什么准备工作？
    2. 输出结果是什么样的？
2. 在GitHub上搜索提供hubconf.py文件的项目。
    1. 返回了多少个仓库？
    2. 找到一个带有hubconf.py的有趣项目。您可以从文档中了解该项目的目的吗？
    3. 将项目加入到书签，并在完成本书后再来回看。你能理解该项目的实现吗？

## 2.7 <span id='chap2-7'>小结</span>

* 预训练网络是一种已经在数据集上训练过的模型。这样的网络通常可以在加载网络参数后立即产生有用的结果。
* 通过了解如何使用预训练模型，我们可以将神经网络集成到项目中，而无需设计或训练它。
* AlexNet和ResNet是两个深度卷积网络，它们在发布后的几年里为图像识别树立了新的基准。
* 生成式对抗网络（GANs）有两个部分--**生成器**和**判别器**--共同工作以产生与真实物品无法区分的输出。
* CycleGAN使用的架构支持在两个不同类别的图像之间来回转换。
* NeuralTalk2使用混合模型架构来消费图像并产生图像的文本描述。
* Torch Hub是使用适当的hubconf.py文件从任何项目加载模型和权重的标准化方法。

---
<span id = "jump2-4"> <sup>4</sup></span> 联系发布者，获取特许使用的机会!